{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s9IS9B9-yUU5"
   },
   "source": [
    "# Question 2: DC-GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z-6MQhMOlHXD",
    "outputId": "3b7726da-11ba-4944-c088-69b4d3d7df7c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.10.0+cu111)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.11.1+cu111)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.10.0.2)\n",
      "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.19.5)\n",
      "Requirement already satisfied: imageio in /usr/local/lib/python3.7/dist-packages (2.4.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from imageio) (1.19.5)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from imageio) (7.1.2)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (3.2.2)\n",
      "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.19.5)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (3.0.7)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib) (1.15.0)\n",
      "/content/Question2\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision\n",
    "!pip install imageio\n",
    "!pip install matplotlib\n",
    "%mkdir -p /content/Question2/\n",
    "%cd /content/Question2/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uwPXHpypDPeS",
    "outputId": "4ad9aee8-e19d-4747-da70-09151c5b6211"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bmL7TiUJCxfr"
   },
   "source": [
    "# Importing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zo-ed8OeC0GF"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import Parameter\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "import tarfile\n",
    "import imageio\n",
    "from urllib.error import URLError\n",
    "from urllib.error import HTTPError\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4BIpGwANoQOg"
   },
   "source": [
    "## Helper Functions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D-UJHBYZkh7f"
   },
   "outputs": [],
   "source": [
    "class AttrDict(dict):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(AttrDict, self).__init__(*args, **kwargs)\n",
    "        self.__dict__ = self\n",
    "\n",
    "                \n",
    "def to_var(tensor, cuda=True):\n",
    "    if cuda:\n",
    "        return Variable(tensor.cuda())\n",
    "    else:\n",
    "        return Variable(tensor)\n",
    "\n",
    "    \n",
    "def to_data(x):\n",
    "    if torch.cuda.is_available():\n",
    "        x = x.cpu()\n",
    "    return x.data.numpy()\n",
    "\n",
    "\n",
    "def create_dir(directory):\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "\n",
    "def gan_checkpoint(iteration, G, D, opts):\n",
    "    G_path = os.path.join(opts.checkpoint_dir, 'G.pkl')\n",
    "    D_path = os.path.join(opts.checkpoint_dir, 'D.pkl')\n",
    "    torch.save(G.state_dict(), G_path)\n",
    "    torch.save(D.state_dict(), D_path)\n",
    "\n",
    "\n",
    "def merge_images(sources, targets, opts):\n",
    "    _, _, h, w = sources.shape\n",
    "    row = int(np.sqrt(opts.batch_size))\n",
    "    merged = np.zeros([3, row * h, row * w * 2])\n",
    "    for (idx, s, t) in (zip(range(row ** 2), sources, targets, )):\n",
    "        i = idx // row\n",
    "        j = idx % row\n",
    "        merged[:, i * h:(i + 1) * h, (j * 2) * h:(j * 2 + 1) * h] = s\n",
    "        merged[:, i * h:(i + 1) * h, (j * 2 + 1) * h:(j * 2 + 2) * h] = t\n",
    "    return merged.transpose(1, 2, 0)\n",
    "\n",
    "\n",
    "def generate_gif(directory_path, keyword=None):\n",
    "    images = []\n",
    "    for filename in sorted(os.listdir(directory_path)):\n",
    "        if filename.endswith(\".png\") and (keyword is None or keyword in filename):\n",
    "            img_path = os.path.join(directory_path, filename)\n",
    "            print(\"adding image {}\".format(img_path))\n",
    "            images.append(imageio.imread(img_path))\n",
    "\n",
    "    if keyword:\n",
    "        imageio.mimsave(\n",
    "            os.path.join(directory_path, 'anim_{}.gif'.format(keyword)), images)\n",
    "    else:\n",
    "        imageio.mimsave(os.path.join(directory_path, 'anim.gif'), images)\n",
    "\n",
    "\n",
    "def create_image_grid(array, ncols=None):\n",
    "    num_images, channels, cell_h, cell_w = array.shape\n",
    "    if not ncols:\n",
    "        ncols = int(np.sqrt(num_images))\n",
    "    nrows = int(np.math.floor(num_images / float(ncols)))\n",
    "    result = np.zeros((cell_h * nrows, cell_w * ncols, channels), dtype=array.dtype)\n",
    "    for i in range(0, nrows):\n",
    "        for j in range(0, ncols):\n",
    "            result[i * cell_h:(i + 1) * cell_h, j * cell_w:(j + 1) * cell_w, :] = array[i * ncols + j].transpose(1, 2,\n",
    "                                                                                                                 0)\n",
    "\n",
    "    if channels == 1:\n",
    "        result = result.squeeze()\n",
    "    return result\n",
    "\n",
    "\n",
    "def gan_save_samples(G, fixed_noise, iteration, opts):\n",
    "    generated_images = G(fixed_noise)\n",
    "    generated_images = to_data(generated_images)\n",
    "\n",
    "    grid = create_image_grid(generated_images)\n",
    "\n",
    "    path = os.path.join(opts.sample_dir, 'sample-{:06d}.png'.format(iteration))\n",
    "    imageio.imwrite(path, grid)\n",
    "    print('Saved {}'.format(path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pbvpn4MaV0I1"
   },
   "source": [
    "## Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XVT4TNTOV3Eg"
   },
   "outputs": [],
   "source": [
    "def get_emoji_loader(emoji_type, opts):\n",
    "    transform = transforms.Compose([\n",
    "                    transforms.Scale(opts.image_size),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "                ])\n",
    "\n",
    "    train_path = os.path.join('data/emojis', emoji_type)\n",
    "    test_path = os.path.join('data/emojis', 'Test_{}'.format(emoji_type))\n",
    "\n",
    "    train_dataset = datasets.ImageFolder(train_path, transform)\n",
    "    test_dataset = datasets.ImageFolder(test_path, transform)\n",
    "\n",
    "    train_dloader = DataLoader(dataset=train_dataset, batch_size=opts.batch_size, shuffle=True, num_workers=opts.num_workers)\n",
    "    test_dloader = DataLoader(dataset=test_dataset, batch_size=opts.batch_size, shuffle=False, num_workers=opts.num_workers)\n",
    "\n",
    "    return train_dloader, test_dloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u0KxX0sDpXKb"
   },
   "source": [
    "## Helper modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y7s0etAmpUgT"
   },
   "outputs": [],
   "source": [
    "# Generates a PyTorch Tensor of uniform random noise.\n",
    "def sample_noise(batch_size, dim):\n",
    "    return to_var(torch.rand(batch_size, dim) * 2 - 1).unsqueeze(2).unsqueeze(3)\n",
    "  \n",
    "\n",
    "def upsampling(in_channels, out_channels, kernel_size, stride=2, padding=2, batch_norm=True, spectral_norm=False):\n",
    "    layers = []\n",
    "    if stride>1:\n",
    "        layers.append(nn.Upsample(scale_factor=stride))\n",
    "    conv_layer = nn.Conv2d(in_channels=in_channels, out_channels=out_channels,\n",
    "                           kernel_size=kernel_size, stride=1, padding=padding, bias=False)\n",
    "    if spectral_norm:\n",
    "        layers.append(SpectralNorm(conv_layer))\n",
    "    else:\n",
    "        layers.append(conv_layer)\n",
    "    if batch_norm:\n",
    "        layers.append(nn.BatchNorm2d(out_channels))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "def conv(in_channels, out_channels, kernel_size, stride=2, padding=2, batch_norm=True, init_zero_weights=False, spectral_norm=False):\n",
    "    layers = []\n",
    "    conv_layer = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding, bias=False)\n",
    "    if init_zero_weights:\n",
    "        conv_layer.weight.data = torch.randn(out_channels, in_channels, kernel_size, kernel_size) * 0.001\n",
    "            \n",
    "    if spectral_norm:\n",
    "        layers.append(SpectralNorm(conv_layer))\n",
    "    else:\n",
    "        layers.append(conv_layer)\n",
    "\n",
    "    if batch_norm:\n",
    "        layers.append(nn.BatchNorm2d(out_channels))\n",
    "    return nn.Sequential(*layers)\n",
    "  \n",
    "\n",
    "class ResnetBlock(nn.Module):\n",
    "    def __init__(self, conv_dim):\n",
    "        super(ResnetBlock, self).__init__()\n",
    "        self.conv_layer = conv(in_channels=conv_dim, out_channels=conv_dim, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x + self.conv_layer(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S0_YbBwe5k35"
   },
   "source": [
    "## DCGAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m1E_jDaBLT1P"
   },
   "source": [
    "## Spectral Norm class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2hy97i1-LT1Q"
   },
   "outputs": [],
   "source": [
    "def l2normalize(v, eps=1e-12):\n",
    "    return v / (v.norm() + eps)\n",
    "\n",
    "\n",
    "class SpectralNorm(nn.Module):\n",
    "    def __init__(self, module, name='weight', power_iterations=1):\n",
    "        super(SpectralNorm, self).__init__()\n",
    "        self.module = module\n",
    "        self.name = name\n",
    "        self.power_iterations = power_iterations\n",
    "        if not self._made_params():\n",
    "            self._make_params()\n",
    "\n",
    "    def _update_u_v(self):\n",
    "        u = getattr(self.module, self.name + \"_u\")\n",
    "        v = getattr(self.module, self.name + \"_v\")\n",
    "        w = getattr(self.module, self.name + \"_bar\")\n",
    "\n",
    "        height = w.data.shape[0]\n",
    "        for _ in range(self.power_iterations):\n",
    "            v.data = l2normalize(torch.mv(torch.t(w.view(height,-1).data), u.data))\n",
    "            u.data = l2normalize(torch.mv(w.view(height,-1).data, v.data))\n",
    "\n",
    "        sigma = u.dot(w.view(height, -1).mv(v))\n",
    "        setattr(self.module, self.name, w / sigma.expand_as(w))\n",
    "\n",
    "    def _made_params(self):\n",
    "        try:\n",
    "            u = getattr(self.module, self.name + \"_u\")\n",
    "            v = getattr(self.module, self.name + \"_v\")\n",
    "            w = getattr(self.module, self.name + \"_bar\")\n",
    "            return True\n",
    "        except AttributeError:\n",
    "            return False\n",
    "\n",
    "    def _make_params(self):\n",
    "        w = getattr(self.module, self.name)\n",
    "\n",
    "        height = w.data.shape[0]\n",
    "        width = w.view(height, -1).data.shape[1]\n",
    "\n",
    "        u = Parameter(w.data.new(height).normal_(0, 1), requires_grad=False)\n",
    "        v = Parameter(w.data.new(width).normal_(0, 1), requires_grad=False)\n",
    "        u.data = l2normalize(u.data)\n",
    "        v.data = l2normalize(v.data)\n",
    "        w_bar = Parameter(w.data)\n",
    "\n",
    "        del self.module._parameters[self.name]\n",
    "\n",
    "        self.module.register_parameter(self.name + \"_u\", u)\n",
    "        self.module.register_parameter(self.name + \"_v\", v)\n",
    "        self.module.register_parameter(self.name + \"_bar\", w_bar)\n",
    "\n",
    "    def forward(self, *args):\n",
    "        self._update_u_v()\n",
    "        return self.module.forward(*args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_BAfi_8yWB3y"
   },
   "source": [
    "## GAN generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ms7DJj3xlTV9"
   },
   "source": [
    "### Deconvolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_ubVuHm2kU9u"
   },
   "outputs": [],
   "source": [
    "class DCGenerator_Deconvolution(nn.Module):\n",
    "    def __init__(self, noise_size, conv_dim, spectral_norm=False):\n",
    "        super(DCGenerator_Deconvolution, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            # Input is 1 x 64 x 64\n",
    "            nn.Conv2d(1, 64, (4, 4), (2, 2), (1, 1), bias=True),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            # State size. 64 x 32 x 32\n",
    "            nn.Conv2d(64, 128, (4, 4), (2, 2), (1, 1), bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            # State size. 128 x 16 x 16\n",
    "            nn.Conv2d(128, 256, (4, 4), (2, 2), (1, 1), bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            # State size. 256 x 8 x 8\n",
    "            nn.Conv2d(256, 512, (4, 4), (2, 2), (1, 1), bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            # State size. 512 x 4 x 4\n",
    "            nn.Conv2d(512, 1, (4, 4), (1, 1), (0, 0), bias=True),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.main(x)\n",
    "        out = torch.flatten(out, 1)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9ztmyA5Ro67o"
   },
   "outputs": [],
   "source": [
    "class DCGenerator_Upsampling(nn.Module):\n",
    "    def __init__(self, noise_size, conv_dim, spectral_norm=False):\n",
    "        super(DCGenerator_Upsampling, self).__init__()\n",
    "\n",
    "        self.conv_dim = conv_dim\n",
    "\n",
    "        self.linear_bn = upsampling(in_channels=100, out_channels=self.conv_dim*4,\n",
    "                                    kernel_size=3, stride=2, padding=2, spectral_norm=spectral_norm)\n",
    "        self.upsampling1 = upsampling(in_channels=self.conv_dim*4, out_channels=self.conv_dim*2,\n",
    "                                      kernel_size=5, stride=2, spectral_norm=spectral_norm)\n",
    "        self.upsampling2 = upsampling(in_channels=self.conv_dim*2, out_channels=self.conv_dim,\n",
    "                                      kernel_size=5, stride=2, spectral_norm=spectral_norm)\n",
    "        self.upsampling3 = upsampling(in_channels=self.conv_dim, out_channels=3,\n",
    "                                      kernel_size=5, stride=2, spectral_norm=spectral_norm)\n",
    "\n",
    "    def forward(self, z):\n",
    "        batch_size = z.size(0)\n",
    "\n",
    "        out = F.relu(self.linear_bn(z)).view(-1, self.conv_dim*4, 4, 4)    # BS x 128 x 4 x 4\n",
    "        out = F.relu(self.upsampling1(out))  # BS x 64 x 8 x 8\n",
    "        out = F.relu(self.upsampling2(out))  # BS x 32 x 16 x 16\n",
    "        out = F.tanh(self.upsampling3(out))  # BS x 3 x 32 x 32\n",
    "        \n",
    "        out_size = out.size()\n",
    "        if out_size != torch.Size([batch_size, 3, 32, 32]):\n",
    "            raise ValueError(\"expect {} x 3 x 32 x 32, but get {}\".format(batch_size, out_size))\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cG4uqAVPp8_B"
   },
   "source": [
    "## GAN discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0GkjXydnqARR"
   },
   "outputs": [],
   "source": [
    "class DCDiscriminator(nn.Module):\n",
    "    def __init__(self, conv_dim=64, spectral_norm=False):\n",
    "        super(DCDiscriminator, self).__init__()\n",
    "\n",
    "        self.conv1 = conv(in_channels=3, out_channels=conv_dim, kernel_size=5, stride=2, spectral_norm=spectral_norm)\n",
    "        self.conv2 = conv(in_channels=conv_dim, out_channels=conv_dim*2, kernel_size=5, stride=2, \n",
    "                          spectral_norm=spectral_norm)\n",
    "        self.conv3 = conv(in_channels=conv_dim*2, out_channels=conv_dim*4, kernel_size=5, \n",
    "                          stride=2, spectral_norm=spectral_norm)\n",
    "        self.conv4 = conv(in_channels=conv_dim*4, out_channels=1, kernel_size=5, stride=2, padding=1, \n",
    "                          batch_norm=False, spectral_norm=spectral_norm)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        out = F.relu(self.conv1(x))\n",
    "        out = F.relu(self.conv2(out))\n",
    "        out = F.relu(self.conv3(out))\n",
    "\n",
    "        out = self.conv4(out).squeeze()\n",
    "        out_size = out.size()\n",
    "        if out_size != torch.Size([batch_size,]):\n",
    "            raise ValueError(\"expect {} x 1, but get {}\".format(batch_size, out_size))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zArqx5mZG11Q"
   },
   "outputs": [],
   "source": [
    "\n",
    "def print_models(G_XtoY, G_YtoX, D_X, D_Y):\n",
    "    if G_YtoX:\n",
    "        print(\"                 G_XtoY                \")\n",
    "        print(\"---------------------------------------\")\n",
    "        print(G_XtoY)\n",
    "        print(\"---------------------------------------\")\n",
    "\n",
    "        print(\"                 G_YtoX                \")\n",
    "        print(\"---------------------------------------\")\n",
    "        print(G_YtoX)\n",
    "        print(\"---------------------------------------\")\n",
    "\n",
    "        print(\"                  D_X                  \")\n",
    "        print(\"---------------------------------------\")\n",
    "        print(D_X)\n",
    "        print(\"---------------------------------------\")\n",
    "\n",
    "        print(\"                  D_Y                  \")\n",
    "        print(\"---------------------------------------\")\n",
    "        print(D_Y)\n",
    "        print(\"---------------------------------------\")\n",
    "    else:\n",
    "        print(\"                 G                     \")\n",
    "        print(\"---------------------------------------\")\n",
    "        print(G_XtoY)\n",
    "        print(\"---------------------------------------\")\n",
    "\n",
    "        print(\"                  D                    \")\n",
    "        print(\"---------------------------------------\")\n",
    "        print(D_X)\n",
    "        print(\"---------------------------------------\")\n",
    "\n",
    "\n",
    "def create_model(opts):\n",
    "    \"\"\"Builds the generators and discriminators.\n",
    "    \"\"\"\n",
    "    if opts.Y is None:\n",
    "        ### DC-GAN with upsampling\n",
    "        G = DCGenerator_Upsampling(noise_size=opts.noise_size, conv_dim=opts.g_conv_dim, spectral_norm=opts.spectral_norm)\n",
    "        D = DCDiscriminator(conv_dim=opts.d_conv_dim, spectral_norm=opts.spectral_norm)\n",
    "\n",
    "        print_models(G, None, D, None)\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            G.cuda()\n",
    "            D.cuda()\n",
    "            print('Models moved to GPU.')\n",
    "        return G, D\n",
    "    else:\n",
    "        ### DC-GAN with deconvolution\n",
    "        G = DCGenerator_Deconvolution(noise_size=opts.noise_size, conv_dim=opts.g_conv_dim, spectral_norm=opts.spectral_norm)\n",
    "        D = DCDiscriminator(conv_dim=opts.d_conv_dim, spectral_norm=opts.spectral_norm)\n",
    "\n",
    "        print_models(G, None, D, None)\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            G.cuda()\n",
    "            D.cuda()\n",
    "            print('Models moved to GPU.')\n",
    "        return G, D\n",
    "\n",
    "def train(opts):\n",
    "    \"\"\"Loads the data, creates checkpoint and sample directories, and starts the training loop.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create train and test dataloaders for images from the two domains X and Y\n",
    "    dataloader_X, test_dataloader_X = get_emoji_loader(emoji_type=opts.X, opts=opts)\n",
    "    if opts.Y:\n",
    "        dataloader_Y, test_dataloader_Y = get_emoji_loader(emoji_type=opts.Y, opts=opts)\n",
    "\n",
    "    # Create checkpoint and sample directories\n",
    "    create_dir(opts.checkpoint_dir)\n",
    "    create_dir(opts.sample_dir)\n",
    "\n",
    "    # Start training\n",
    "    if opts.Y is None:\n",
    "        G, D = gan_training_loop(dataloader_X, test_dataloader_X, opts)\n",
    "        return G, D\n",
    "\n",
    "def print_opts(opts):\n",
    "    \"\"\"Prints the values of all command-line arguments.\n",
    "    \"\"\"\n",
    "    print('=' * 80)\n",
    "    print('Opts'.center(80))\n",
    "    print('-' * 80)\n",
    "    for key in opts.__dict__:\n",
    "        if opts.__dict__[key]:\n",
    "            print('{:>30}: {:<30}'.format(key, opts.__dict__[key]).center(80))\n",
    "    print('=' * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G8RtBMu55ysm"
   },
   "source": [
    "### GAN training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MxIJ2Zua51KI"
   },
   "outputs": [],
   "source": [
    "def gan_training_loop(dataloader, test_dataloader, opts):\n",
    "    # Create generators and discriminators\n",
    "    G, D = create_model(opts)\n",
    "\n",
    "    g_params = G.parameters()  # Get generator parameters\n",
    "    d_params = D.parameters()  # Get discriminator parameters\n",
    "\n",
    "    # Create optimizers for the generators and discriminators\n",
    "    g_optimizer = optim.Adam(g_params, opts.lr, [opts.beta1, opts.beta2])\n",
    "    d_optimizer = optim.Adam(d_params, opts.lr * 2., [opts.beta1, opts.beta2])\n",
    "\n",
    "    train_iter = iter(dataloader)\n",
    "\n",
    "    test_iter = iter(test_dataloader)\n",
    "\n",
    "    # Get some fixed data from domains X and Y for sampling. These are images that are held\n",
    "    # constant throughout training, that allow us to inspect the model's performance.\n",
    "    fixed_noise = sample_noise(100, opts.noise_size)  # # 100 x noise_size x 1 x 1\n",
    "\n",
    "    iter_per_epoch = len(train_iter)\n",
    "    total_train_iters = opts.train_iters\n",
    "\n",
    "    losses = {\"iteration\": [], \"D_fake_loss\": [], \"D_real_loss\": [], \"G_loss\": []}\n",
    "\n",
    "    gp_weight = 10\n",
    "\n",
    "    try:\n",
    "        for iteration in range(1, opts.train_iters + 1):\n",
    "\n",
    "            # Reset data_iter for each epoch\n",
    "            if iteration % iter_per_epoch == 0:\n",
    "                train_iter = iter(dataloader)\n",
    "\n",
    "            real_images, real_labels = train_iter.next()\n",
    "            real_images, real_labels = to_var(real_images), to_var(real_labels).long().squeeze()\n",
    "\n",
    "            for d_i in range(opts.d_train_iters):\n",
    "                d_optimizer.zero_grad()\n",
    "\n",
    "                # 1. Compute the discriminator loss on real images\n",
    "                D_real_loss = torch.mean((D(real_images) - 1)**2) \n",
    "\n",
    "                # 2. Sample noise\n",
    "                noise = sample_noise(real_images.shape[0], opts.noise_size)\n",
    "\n",
    "                # 3. Generate fake images from the noise\n",
    "                fake_images = G(noise)\n",
    "                \n",
    "                # 4. Compute the discriminator loss on the fake images\n",
    "                D_fake_loss = torch.mean(D(fake_images)**2)\n",
    "\n",
    "                # ---- Gradient Penalty ----\n",
    "                if opts.gradient_penalty:\n",
    "                    alpha = torch.rand(real_images.shape[0], 1, 1, 1)\n",
    "                    alpha = alpha.expand_as(real_images).cuda()\n",
    "                    interp_images = Variable(alpha * real_images.data + alpha * fake_images.data, requires_grad=True).cuda()\n",
    "                    D_interp_output = D(interp_images)\n",
    "\n",
    "                    gradients = torch.autograd.grad(outputs=D_interp_output, inputs=interp_images,\n",
    "                                                    grad_outputs=torch.ones(D_interp_output.size()).cuda(),\n",
    "                                                    create_graph=True, retain_graph=True)[0]\n",
    "                    gradients = gradients.view(real_images.shape[0], -1)\n",
    "                    gradients_norm = torch.sqrt(torch.sum(gradients ** 2, dim=1) + 1e-12)\n",
    "\n",
    "                    gp = gp_weight * gradients_norm.mean()\n",
    "                else:\n",
    "                    gp = 0.0\n",
    "\n",
    "                # 5. Compute the total discriminator loss\n",
    "                D_total_loss = D_real_loss + D_fake_loss\n",
    "\n",
    "                D_total_loss.backward()\n",
    "                d_optimizer.step()\n",
    "\n",
    "\n",
    "            g_optimizer.zero_grad()\n",
    "\n",
    "            # 1. Sample noise\n",
    "            noise = sample_noise(real_images.shape[0], opts.noise_size)\n",
    "\n",
    "            # 2. Generate fake images from the noise\n",
    "            fake_images = G(noise)\n",
    "\n",
    "            # 3. Compute the generator loss\n",
    "            G_loss = torch.mean((D(fake_images)-1)**2)\n",
    "\n",
    "            G_loss.backward()\n",
    "            g_optimizer.step()\n",
    "\n",
    "            # Print the log info\n",
    "            if iteration % opts.log_step == 0:\n",
    "                losses['iteration'].append(iteration)\n",
    "                losses['D_real_loss'].append(D_real_loss.item())\n",
    "                losses['D_fake_loss'].append(D_fake_loss.item())\n",
    "                losses['G_loss'].append(G_loss.item())\n",
    "                print('Iteration [{:4d}/{:4d}] | D_real_loss: {:6.4f} | D_fake_loss: {:6.4f} | G_loss: {:6.4f}'.format(\n",
    "                    iteration, total_train_iters, D_real_loss.item(), D_fake_loss.item(), G_loss.item()))\n",
    "\n",
    "            # Save the generated samples\n",
    "            if iteration % opts.sample_every == 0:\n",
    "                gan_save_samples(G, fixed_noise, iteration, opts)\n",
    "\n",
    "            # Save the model parameters\n",
    "            if iteration % opts.checkpoint_every == 0:\n",
    "                gan_checkpoint(iteration, G, D, opts)\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print('Exiting early from training.')\n",
    "        return G, D\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(losses['iteration'], losses['D_real_loss'], label='D_real')\n",
    "    plt.plot(losses['iteration'], losses['D_fake_loss'], label='D_fake')\n",
    "    plt.plot(losses['iteration'], losses['G_loss'], label='G')\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(opts.sample_dir, 'losses.png'))\n",
    "    plt.close()\n",
    "    return G, D\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XuNFd6LNo0-o"
   },
   "source": [
    "# Training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kiUwiOITHTW4"
   },
   "source": [
    "## Download dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hiv5d_PlD02S",
    "outputId": "15a3458a-559d-40e7-bd50-189915bbcaa3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘data/’: File exists\n",
      "mkdir: cannot create directory ‘data/emojis/’: File exists\n"
     ]
    }
   ],
   "source": [
    "!mkdir data/\n",
    "!mkdir data/emojis/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xwcFjsEpHRbI"
   },
   "outputs": [],
   "source": [
    "!unzip /content/drive/MyDrive/emojis.zip -d data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hmQmyJDSRFKR"
   },
   "source": [
    "# Part A: Implementing DC-GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0LKaRF1jwhH7",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SEED = 11\n",
    "\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "\n",
    "args = AttrDict()\n",
    "args_dict = {\n",
    "              'image_size':32, \n",
    "              'g_conv_dim':32, \n",
    "              'd_conv_dim':64,\n",
    "              'noise_size':100,\n",
    "              'num_workers': 0,\n",
    "              'train_iters':20000,\n",
    "              'X':'Windows',  # options: 'Windows' / 'Apple'\n",
    "              'Y': 'Deconvolution',\n",
    "              # Optimize Options\n",
    "              'lr':0.0003,\n",
    "              'beta1':0.5,\n",
    "              'beta2':0.999,\n",
    "             \n",
    "              'batch_size':32, \n",
    "              'checkpoint_dir': 'results/checkpoints_gan',\n",
    "              'sample_dir': 'results/samples_gan',\n",
    "              'load': None,\n",
    "              'log_step':200,\n",
    "              'sample_every':200,\n",
    "              'checkpoint_every':1000,\n",
    "              'spectral_norm': False,\n",
    "              'gradient_penalty': False,\n",
    "              'd_train_iters': 1\n",
    "}\n",
    "args.update(args_dict)\n",
    "\n",
    "print_opts(args)\n",
    "G, D = train(args)\n",
    "\n",
    "generate_gif(\"results/samples_gan\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GDC-NA4NLEzc"
   },
   "source": [
    "# Part B: Gradient Penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pQ27QscTMwsw"
   },
   "outputs": [],
   "source": [
    "!mkdir result_B\n",
    "!mkdir result_B/checkpoints_gan\n",
    "!mkdir result_B/samples_gan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5UpVkHblGKWS"
   },
   "outputs": [],
   "source": [
    "SEED = 11\n",
    "\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "\n",
    "args = AttrDict()\n",
    "args_dict = {\n",
    "              'image_size':32, \n",
    "              'g_conv_dim':32, \n",
    "              'd_conv_dim':64,\n",
    "              'noise_size':100,\n",
    "              'num_workers': 0,\n",
    "              'train_iters':20000,\n",
    "              'X':'Windows',  # options: 'Windows' / 'Apple'\n",
    "              'Y': 'Deconvolution',\n",
    "              # Optimize Options\n",
    "              'lr':0.00001,\n",
    "              'beta1':0.4,\n",
    "              'beta2':0.999,\n",
    "             \n",
    "              'batch_size':32, \n",
    "              'checkpoint_dir': 'result_B/checkpoints_gan',\n",
    "              'sample_dir': 'result_B/samples_gan',\n",
    "              'load': None,\n",
    "              'log_step':200,\n",
    "              'sample_every':200,\n",
    "              'checkpoint_every':1000,\n",
    "              'spectral_norm': True,\n",
    "              'gradient_penalty': True,\n",
    "              'd_train_iters': 1\n",
    "}\n",
    "args.update(args_dict)\n",
    "\n",
    "print_opts(args)\n",
    "G, D = train(args)\n",
    "\n",
    "generate_gif(\"result_B/samples_gan\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ROiBjeRMlK5"
   },
   "source": [
    "# Part C:Trying different hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HBiWmecyMsZc"
   },
   "outputs": [],
   "source": [
    "!mkdir result_C\n",
    "!mkdir result_C/checkpoints_gan\n",
    "!mkdir result_C/samples_gan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9XfSKKzGhDAA"
   },
   "outputs": [],
   "source": [
    "SEED = 11\n",
    "\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "\n",
    "args = AttrDict()\n",
    "args_dict = {\n",
    "              'image_size':32, \n",
    "              'g_conv_dim':32, \n",
    "              'd_conv_dim':64,\n",
    "              'noise_size':100,\n",
    "              'num_workers': 0,\n",
    "              'train_iters':20000,\n",
    "              'X':'Windows',  # options: 'Windows' / 'Apple'\n",
    "              'Y': 'Deconvolution',\n",
    "              # Optimize Options\n",
    "              'lr':0.0001,\n",
    "              'beta1':0.6,\n",
    "              'beta2':0.999,\n",
    "              'batch_size':32, \n",
    "              'checkpoint_dir': 'result_B/checkpoints_gan',\n",
    "              'sample_dir': 'result_B/samples_gan',\n",
    "              'load': None,\n",
    "              'log_step':200,\n",
    "              'sample_every':200,\n",
    "              'checkpoint_every':1000,\n",
    "              'spectral_norm': True,\n",
    "              'gradient_penalty': True,\n",
    "              'd_train_iters': 1,\n",
    "}\n",
    "args.update(args_dict)\n",
    "\n",
    "print_opts(args)\n",
    "G, D = train(args)\n",
    "\n",
    "generate_gif(\"result_B/samples_gan\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C3GXMzYkP6l7"
   },
   "source": [
    "# Part D: Upsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IrJ2Qb7qP9ZA"
   },
   "outputs": [],
   "source": [
    "!mkdir result_D\n",
    "!mkdir result_D/checkpoints_gan\n",
    "!mkdir result_D/samples_gan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GKhP4bqQP9ZA",
    "outputId": "e0dfb8a6-5771-4073-d7b7-388a57752b6a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "                                      Opts                                      \n",
      "--------------------------------------------------------------------------------\n",
      "                             image_size: 32                                     \n",
      "                             g_conv_dim: 32                                     \n",
      "                             d_conv_dim: 64                                     \n",
      "                             noise_size: 100                                    \n",
      "                            train_iters: 20000                                  \n",
      "                                      X: Windows                                \n",
      "                                     lr: 0.0003                                 \n",
      "                                  beta1: 0.5                                    \n",
      "                                  beta2: 0.999                                  \n",
      "                             batch_size: 32                                     \n",
      "                         checkpoint_dir: result_C/checkpoints_gan               \n",
      "                             sample_dir: result_C/samples_gan                   \n",
      "                               log_step: 200                                    \n",
      "                           sample_every: 200                                    \n",
      "                       checkpoint_every: 1000                                   \n",
      "                          spectral_norm: 1                                      \n",
      "                       gradient_penalty: 1                                      \n",
      "                          d_train_iters: 1                                      \n",
      "================================================================================\n",
      "                 G                     \n",
      "---------------------------------------\n",
      "DCGenerator_Upsampling(\n",
      "  (linear_bn): Sequential(\n",
      "    (0): Upsample(scale_factor=2.0, mode=nearest)\n",
      "    (1): SpectralNorm(\n",
      "      (module): Conv2d(100, 128, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), bias=False)\n",
      "    )\n",
      "    (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (upsampling1): Sequential(\n",
      "    (0): Upsample(scale_factor=2.0, mode=nearest)\n",
      "    (1): SpectralNorm(\n",
      "      (module): Conv2d(128, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
      "    )\n",
      "    (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (upsampling2): Sequential(\n",
      "    (0): Upsample(scale_factor=2.0, mode=nearest)\n",
      "    (1): SpectralNorm(\n",
      "      (module): Conv2d(64, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
      "    )\n",
      "    (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (upsampling3): Sequential(\n",
      "    (0): Upsample(scale_factor=2.0, mode=nearest)\n",
      "    (1): SpectralNorm(\n",
      "      (module): Conv2d(32, 3, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
      "    )\n",
      "    (2): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      ")\n",
      "---------------------------------------\n",
      "                  D                    \n",
      "---------------------------------------\n",
      "DCDiscriminator(\n",
      "  (conv1): Sequential(\n",
      "    (0): SpectralNorm(\n",
      "      (module): Conv2d(3, 64, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), bias=False)\n",
      "    )\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (conv2): Sequential(\n",
      "    (0): SpectralNorm(\n",
      "      (module): Conv2d(64, 128, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), bias=False)\n",
      "    )\n",
      "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (conv3): Sequential(\n",
      "    (0): SpectralNorm(\n",
      "      (module): Conv2d(128, 256, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), bias=False)\n",
      "    )\n",
      "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (conv4): Sequential(\n",
      "    (0): SpectralNorm(\n",
      "      (module): Conv2d(256, 1, kernel_size=(5, 5), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "---------------------------------------\n",
      "Models moved to GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Lossy conversion from float32 to uint8. Range [-0.9992960095405579, 1.0]. Convert image to uint8 prior to saving to suppress this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration [ 200/20000] | D_real_loss: 0.1648 | D_fake_loss: 0.1237 | G_loss: 0.6326\n",
      "Saved result_C/samples_gan/sample-000200.png\n",
      "Exiting early from training.\n",
      "adding image result_C/samples_gan/losses.png\n",
      "adding image result_C/samples_gan/sample-000200.png\n",
      "adding image result_C/samples_gan/sample-000400.png\n",
      "adding image result_C/samples_gan/sample-000600.png\n",
      "adding image result_C/samples_gan/sample-000800.png\n",
      "adding image result_C/samples_gan/sample-001000.png\n",
      "adding image result_C/samples_gan/sample-001200.png\n",
      "adding image result_C/samples_gan/sample-001400.png\n",
      "adding image result_C/samples_gan/sample-001600.png\n",
      "adding image result_C/samples_gan/sample-001800.png\n",
      "adding image result_C/samples_gan/sample-002000.png\n",
      "adding image result_C/samples_gan/sample-002200.png\n",
      "adding image result_C/samples_gan/sample-002400.png\n",
      "adding image result_C/samples_gan/sample-002600.png\n",
      "adding image result_C/samples_gan/sample-002800.png\n",
      "adding image result_C/samples_gan/sample-003000.png\n",
      "adding image result_C/samples_gan/sample-003200.png\n",
      "adding image result_C/samples_gan/sample-003400.png\n",
      "adding image result_C/samples_gan/sample-003600.png\n",
      "adding image result_C/samples_gan/sample-003800.png\n",
      "adding image result_C/samples_gan/sample-004000.png\n",
      "adding image result_C/samples_gan/sample-004200.png\n",
      "adding image result_C/samples_gan/sample-004400.png\n",
      "adding image result_C/samples_gan/sample-004600.png\n",
      "adding image result_C/samples_gan/sample-004800.png\n",
      "adding image result_C/samples_gan/sample-005000.png\n",
      "adding image result_C/samples_gan/sample-005200.png\n",
      "adding image result_C/samples_gan/sample-005400.png\n",
      "adding image result_C/samples_gan/sample-005600.png\n",
      "adding image result_C/samples_gan/sample-005800.png\n",
      "adding image result_C/samples_gan/sample-006000.png\n",
      "adding image result_C/samples_gan/sample-006200.png\n",
      "adding image result_C/samples_gan/sample-006400.png\n",
      "adding image result_C/samples_gan/sample-006600.png\n",
      "adding image result_C/samples_gan/sample-006800.png\n",
      "adding image result_C/samples_gan/sample-007000.png\n",
      "adding image result_C/samples_gan/sample-007200.png\n",
      "adding image result_C/samples_gan/sample-007400.png\n",
      "adding image result_C/samples_gan/sample-007600.png\n",
      "adding image result_C/samples_gan/sample-007800.png\n",
      "adding image result_C/samples_gan/sample-008000.png\n",
      "adding image result_C/samples_gan/sample-008200.png\n",
      "adding image result_C/samples_gan/sample-008400.png\n",
      "adding image result_C/samples_gan/sample-008600.png\n",
      "adding image result_C/samples_gan/sample-008800.png\n",
      "adding image result_C/samples_gan/sample-009000.png\n",
      "adding image result_C/samples_gan/sample-009200.png\n",
      "adding image result_C/samples_gan/sample-009400.png\n",
      "adding image result_C/samples_gan/sample-009600.png\n",
      "adding image result_C/samples_gan/sample-009800.png\n",
      "adding image result_C/samples_gan/sample-010000.png\n",
      "adding image result_C/samples_gan/sample-010200.png\n",
      "adding image result_C/samples_gan/sample-010400.png\n",
      "adding image result_C/samples_gan/sample-010600.png\n",
      "adding image result_C/samples_gan/sample-010800.png\n",
      "adding image result_C/samples_gan/sample-011000.png\n",
      "adding image result_C/samples_gan/sample-011200.png\n",
      "adding image result_C/samples_gan/sample-011400.png\n",
      "adding image result_C/samples_gan/sample-011600.png\n",
      "adding image result_C/samples_gan/sample-011800.png\n",
      "adding image result_C/samples_gan/sample-012000.png\n",
      "adding image result_C/samples_gan/sample-012200.png\n",
      "adding image result_C/samples_gan/sample-012400.png\n",
      "adding image result_C/samples_gan/sample-012600.png\n",
      "adding image result_C/samples_gan/sample-012800.png\n",
      "adding image result_C/samples_gan/sample-013000.png\n",
      "adding image result_C/samples_gan/sample-013200.png\n",
      "adding image result_C/samples_gan/sample-013400.png\n",
      "adding image result_C/samples_gan/sample-013600.png\n",
      "adding image result_C/samples_gan/sample-013800.png\n",
      "adding image result_C/samples_gan/sample-014000.png\n",
      "adding image result_C/samples_gan/sample-014200.png\n",
      "adding image result_C/samples_gan/sample-014400.png\n",
      "adding image result_C/samples_gan/sample-014600.png\n",
      "adding image result_C/samples_gan/sample-014800.png\n",
      "adding image result_C/samples_gan/sample-015000.png\n",
      "adding image result_C/samples_gan/sample-015200.png\n",
      "adding image result_C/samples_gan/sample-015400.png\n",
      "adding image result_C/samples_gan/sample-015600.png\n",
      "adding image result_C/samples_gan/sample-015800.png\n",
      "adding image result_C/samples_gan/sample-016000.png\n",
      "adding image result_C/samples_gan/sample-016200.png\n",
      "adding image result_C/samples_gan/sample-016400.png\n",
      "adding image result_C/samples_gan/sample-016600.png\n",
      "adding image result_C/samples_gan/sample-016800.png\n",
      "adding image result_C/samples_gan/sample-017000.png\n",
      "adding image result_C/samples_gan/sample-017200.png\n",
      "adding image result_C/samples_gan/sample-017400.png\n",
      "adding image result_C/samples_gan/sample-017600.png\n",
      "adding image result_C/samples_gan/sample-017800.png\n",
      "adding image result_C/samples_gan/sample-018000.png\n",
      "adding image result_C/samples_gan/sample-018200.png\n",
      "adding image result_C/samples_gan/sample-018400.png\n",
      "adding image result_C/samples_gan/sample-018600.png\n",
      "adding image result_C/samples_gan/sample-018800.png\n",
      "adding image result_C/samples_gan/sample-019000.png\n",
      "adding image result_C/samples_gan/sample-019200.png\n",
      "adding image result_C/samples_gan/sample-019400.png\n",
      "adding image result_C/samples_gan/sample-019600.png\n",
      "adding image result_C/samples_gan/sample-019800.png\n",
      "adding image result_C/samples_gan/sample-020000.png\n"
     ]
    }
   ],
   "source": [
    "SEED = 11\n",
    "\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "\n",
    "args = AttrDict()\n",
    "args_dict = {\n",
    "              'image_size':32, \n",
    "              'g_conv_dim':32, \n",
    "              'd_conv_dim':64,\n",
    "              'noise_size':100,\n",
    "              'num_workers': 0,\n",
    "              'train_iters':20000,\n",
    "              'X':'Windows',  # options: 'Windows' / 'Apple'\n",
    "              'Y': None,\n",
    "              # Optimize Options\n",
    "              'lr':0.0003,\n",
    "              'beta1':0.5,\n",
    "              'beta2':0.999,\n",
    "             \n",
    "              'batch_size':32, \n",
    "              'checkpoint_dir': 'result_D/checkpoints_gan',\n",
    "              'sample_dir': 'result_D/samples_gan',\n",
    "              'load': None,\n",
    "              'log_step':200,\n",
    "              'sample_every':200,\n",
    "              'checkpoint_every':1000,\n",
    "              'spectral_norm': True,\n",
    "              'gradient_penalty': True,\n",
    "              'd_train_iters': 1\n",
    "}\n",
    "args.update(args_dict)\n",
    "\n",
    "print_opts(args)\n",
    "G, D = train(args)\n",
    "\n",
    "generate_gif(\"result_D/samples_gan\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "dcgan.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
