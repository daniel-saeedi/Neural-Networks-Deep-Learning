{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5xRJC-XOAYBE"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import Parameter\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "import tarfile\n",
    "\n",
    "import imageio\n",
    "from urllib.error import URLError\n",
    "from urllib.error import HTTPError\n",
    "\n",
    "\n",
    "def get_file(fname,\n",
    "             origin,\n",
    "             untar=False,\n",
    "             extract=False,\n",
    "             archive_format='auto',\n",
    "             cache_dir='data'):\n",
    "    datadir = os.path.join(cache_dir)\n",
    "    if not os.path.exists(datadir):\n",
    "        os.makedirs(datadir)\n",
    "\n",
    "    if untar:\n",
    "        untar_fpath = os.path.join(datadir, fname)\n",
    "        fpath = untar_fpath + '.tar.gz'\n",
    "    else:\n",
    "        fpath = os.path.join(datadir, fname)\n",
    "\n",
    "    print(fpath)\n",
    "    if not os.path.exists(fpath):\n",
    "        print('Downloading data from', origin)\n",
    "\n",
    "        error_msg = 'URL fetch failure on {}: {} -- {}'\n",
    "        try:\n",
    "            try:\n",
    "                urlretrieve(origin, fpath)\n",
    "            except URLError as e:\n",
    "                raise Exception(error_msg.format(origin, e.errno, e.reason))\n",
    "            except HTTPError as e:\n",
    "                raise Exception(error_msg.format(origin, e.code, e.msg))\n",
    "        except (Exception, KeyboardInterrupt) as e:\n",
    "            if os.path.exists(fpath):\n",
    "                os.remove(fpath)\n",
    "            raise\n",
    "\n",
    "    if untar:\n",
    "        if not os.path.exists(untar_fpath):\n",
    "            print('Extracting file.')\n",
    "            with tarfile.open(fpath) as archive:\n",
    "                archive.extractall(datadir)\n",
    "        return untar_fpath\n",
    "\n",
    "    return fpath\n",
    "\n",
    "\n",
    "class AttrDict(dict):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(AttrDict, self).__init__(*args, **kwargs)\n",
    "        self.__dict__ = self\n",
    "\n",
    "                \n",
    "def to_var(tensor, cuda=True):\n",
    "    \"\"\"Wraps a Tensor in a Variable, optionally placing it on the GPU.\n",
    "\n",
    "        Arguments:\n",
    "            tensor: A Tensor object.\n",
    "            cuda: A boolean flag indicating whether to use the GPU.\n",
    "\n",
    "        Returns:\n",
    "            A Variable object, on the GPU if cuda==True.\n",
    "    \"\"\"\n",
    "    if cuda:\n",
    "        return Variable(tensor.cuda())\n",
    "    else:\n",
    "        return Variable(tensor)\n",
    "\n",
    "    \n",
    "def to_data(x):\n",
    "    \"\"\"Converts variable to numpy.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        x = x.cpu()\n",
    "    return x.data.numpy()\n",
    "\n",
    "\n",
    "def create_dir(directory):\n",
    "    \"\"\"Creates a directory if it doesn't already exist.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "\n",
    "def gan_checkpoint(iteration, G, D, opts):\n",
    "    \"\"\"Saves the parameters of the generator G and discriminator D.\n",
    "    \"\"\"\n",
    "    G_path = os.path.join(opts.checkpoint_dir, 'G.pkl')\n",
    "    D_path = os.path.join(opts.checkpoint_dir, 'D.pkl')\n",
    "    torch.save(G.state_dict(), G_path)\n",
    "    torch.save(D.state_dict(), D_path)\n",
    "\n",
    "\n",
    "def cyclegan_checkpoint(iteration, G_XtoY, G_YtoX, D_X, D_Y, opts):\n",
    "    \"\"\"Saves the parameters of both generators G_YtoX, G_XtoY and discriminators D_X, D_Y.\n",
    "    \"\"\"\n",
    "    G_XtoY_path = os.path.join(opts.checkpoint_dir, 'G_XtoY.pkl')\n",
    "    G_YtoX_path = os.path.join(opts.checkpoint_dir, 'G_YtoX.pkl')\n",
    "    D_X_path = os.path.join(opts.checkpoint_dir, 'D_X.pkl')\n",
    "    D_Y_path = os.path.join(opts.checkpoint_dir, 'D_Y.pkl')\n",
    "    torch.save(G_XtoY.state_dict(), G_XtoY_path)\n",
    "    torch.save(G_YtoX.state_dict(), G_YtoX_path)\n",
    "    torch.save(D_X.state_dict(), D_X_path)\n",
    "    torch.save(D_Y.state_dict(), D_Y_path)\n",
    "\n",
    "\n",
    "def load_checkpoint(opts):\n",
    "    \"\"\"Loads the generator and discriminator models from checkpoints.\n",
    "    \"\"\"\n",
    "    G_XtoY_path = os.path.join(opts.load, 'G_XtoY.pkl')\n",
    "    G_YtoX_path = os.path.join(opts.load, 'G_YtoX.pkl')\n",
    "    D_X_path = os.path.join(opts.load, 'D_X.pkl')\n",
    "    D_Y_path = os.path.join(opts.load, 'D_Y.pkl')\n",
    "\n",
    "    G_XtoY = CycleGenerator(conv_dim=opts.g_conv_dim, init_zero_weights=opts.init_zero_weights)\n",
    "    G_YtoX = CycleGenerator(conv_dim=opts.g_conv_dim, init_zero_weights=opts.init_zero_weights)\n",
    "    D_X = DCDiscriminator(conv_dim=opts.d_conv_dim)\n",
    "    D_Y = DCDiscriminator(conv_dim=opts.d_conv_dim)\n",
    "\n",
    "    G_XtoY.load_state_dict(torch.load(G_XtoY_path, map_location=lambda storage, loc: storage))\n",
    "    G_YtoX.load_state_dict(torch.load(G_YtoX_path, map_location=lambda storage, loc: storage))\n",
    "    D_X.load_state_dict(torch.load(D_X_path, map_location=lambda storage, loc: storage))\n",
    "    D_Y.load_state_dict(torch.load(D_Y_path, map_location=lambda storage, loc: storage))\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        G_XtoY.cuda()\n",
    "        G_YtoX.cuda()\n",
    "        D_X.cuda()\n",
    "        D_Y.cuda()\n",
    "        print('Models moved to GPU.')\n",
    "\n",
    "    return G_XtoY, G_YtoX, D_X, D_Y\n",
    "\n",
    "\n",
    "def merge_images(sources, targets, opts):\n",
    "    \"\"\"Creates a grid consisting of pairs of columns, where the first column in\n",
    "    each pair contains images source images and the second column in each pair\n",
    "    contains images generated by the CycleGAN from the corresponding images in\n",
    "    the first column.\n",
    "    \"\"\"\n",
    "    _, _, h, w = sources.shape\n",
    "    row = int(np.sqrt(opts.batch_size))\n",
    "    merged = np.zeros([3, row * h, row * w * 2])\n",
    "    for (idx, s, t) in (zip(range(row ** 2), sources, targets, )):\n",
    "        i = idx // row\n",
    "        j = idx % row\n",
    "        merged[:, i * h:(i + 1) * h, (j * 2) * h:(j * 2 + 1) * h] = s\n",
    "        merged[:, i * h:(i + 1) * h, (j * 2 + 1) * h:(j * 2 + 2) * h] = t\n",
    "    return merged.transpose(1, 2, 0)\n",
    "\n",
    "\n",
    "def generate_gif(directory_path, keyword=None):\n",
    "    images = []\n",
    "    for filename in sorted(os.listdir(directory_path)):\n",
    "        if filename.endswith(\".png\") and (keyword is None or keyword in filename):\n",
    "            img_path = os.path.join(directory_path, filename)\n",
    "            print(\"adding image {}\".format(img_path))\n",
    "            images.append(imageio.imread(img_path))\n",
    "\n",
    "    if keyword:\n",
    "        imageio.mimsave(\n",
    "            os.path.join(directory_path, 'anim_{}.gif'.format(keyword)), images)\n",
    "    else:\n",
    "        imageio.mimsave(os.path.join(directory_path, 'anim.gif'), images)\n",
    "\n",
    "\n",
    "def create_image_grid(array, ncols=None):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    num_images, channels, cell_h, cell_w = array.shape\n",
    "    if not ncols:\n",
    "        ncols = int(np.sqrt(num_images))\n",
    "    nrows = int(np.math.floor(num_images / float(ncols)))\n",
    "    result = np.zeros((cell_h * nrows, cell_w * ncols, channels), dtype=array.dtype)\n",
    "    for i in range(0, nrows):\n",
    "        for j in range(0, ncols):\n",
    "            result[i * cell_h:(i + 1) * cell_h, j * cell_w:(j + 1) * cell_w, :] = array[i * ncols + j].transpose(1, 2,\n",
    "                                                                                                                 0)\n",
    "\n",
    "    if channels == 1:\n",
    "        result = result.squeeze()\n",
    "    return result\n",
    "\n",
    "\n",
    "def gan_save_samples(G, fixed_noise, iteration, opts):\n",
    "    generated_images = G(fixed_noise)\n",
    "    generated_images = to_data(generated_images)\n",
    "\n",
    "    grid = create_image_grid(generated_images)\n",
    "\n",
    "    # merged = merge_images(X, fake_Y, opts)\n",
    "    path = os.path.join(opts.sample_dir, 'sample-{:06d}.png'.format(iteration))\n",
    "    imageio.imwrite(path, grid)\n",
    "    print('Saved {}'.format(path))\n",
    "\n",
    "\n",
    "def cyclegan_save_samples(iteration, fixed_Y, fixed_X, G_YtoX, G_XtoY, opts):\n",
    "    \"\"\"Saves samples from both generators X->Y and Y->X.\n",
    "    \"\"\"\n",
    "    fake_X = G_YtoX(fixed_Y)\n",
    "    fake_Y = G_XtoY(fixed_X)\n",
    "\n",
    "    X, fake_X = to_data(fixed_X), to_data(fake_X)\n",
    "    Y, fake_Y = to_data(fixed_Y), to_data(fake_Y)\n",
    "\n",
    "    merged = merge_images(X, fake_Y, opts)\n",
    "    path = os.path.join(opts.sample_dir, 'sample-{:06d}-X-Y.png'.format(iteration))\n",
    "    imageio.imwrite(path, merged)\n",
    "    print('Saved {}'.format(path))\n",
    "\n",
    "    merged = merge_images(Y, fake_X, opts)\n",
    "    path = os.path.join(opts.sample_dir, 'sample-{:06d}-Y-X.png'.format(iteration))\n",
    "    imageio.imwrite(path, merged)\n",
    "    print('Saved {}'.format(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mNuAXyeDAgyE"
   },
   "outputs": [],
   "source": [
    "def get_emoji_loader(emoji_type, opts):\n",
    "    \"\"\"Creates training and test data loaders.\n",
    "    \"\"\"\n",
    "    transform = transforms.Compose([\n",
    "                    transforms.Scale(32),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "                ])\n",
    "\n",
    "    train_path = os.path.join('data/emojis', emoji_type)\n",
    "    test_path = os.path.join('data/emojis', 'Test_{}'.format(emoji_type))\n",
    "\n",
    "    train_dataset = datasets.ImageFolder(train_path, transform)\n",
    "    test_dataset = datasets.ImageFolder(test_path, transform)\n",
    "\n",
    "    train_dloader = DataLoader(dataset=train_dataset, batch_size=32, shuffle=True, num_workers=2)\n",
    "    test_dloader = DataLoader(dataset=test_dataset, batch_size=32, shuffle=False, num_workers=2)\n",
    "\n",
    "    return train_dloader, test_dloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8hiHtGmyB3ej"
   },
   "outputs": [],
   "source": [
    "args_dict = {\n",
    "              'image_size':32, \n",
    "              'g_conv_dim':32, \n",
    "              'd_conv_dim':64,\n",
    "              'noise_size':100,\n",
    "              'num_workers': 0,\n",
    "              'train_iters':20000,\n",
    "              'X':'Windows',  # options: 'Windows' / 'Apple'\n",
    "              'Y': None,\n",
    "              'lr':0.0003,\n",
    "              'beta1':0.5,\n",
    "              'beta2':0.999,\n",
    "              'batch_size':32, \n",
    "              'checkpoint_dir': 'results/checkpoints_gan',\n",
    "              'sample_dir': 'results/samples_gan',\n",
    "              'load': None,\n",
    "              'log_step':200,\n",
    "              'sample_every':200,\n",
    "              'checkpoint_every':1000,\n",
    "              'spectral_norm': False,\n",
    "              'gradient_penalty': False,\n",
    "              'd_train_iters': 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QbYJHsQJ8d3b",
    "outputId": "e8b6baeb-2697-4139-eca6-18ccc2501b83"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_11 (Dense)            (None, 6272)              457856    \n",
      "                                                                 \n",
      " reshape_2 (Reshape)         (None, 7, 7, 128)         0         \n",
      "                                                                 \n",
      " batch_normalization_15 (Bat  (None, 7, 7, 128)        512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " up_sampling2d_4 (UpSampling  (None, 14, 14, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_18 (Conv2D)          (None, 14, 14, 128)       147584    \n",
      "                                                                 \n",
      " activation_6 (Activation)   (None, 14, 14, 128)       0         \n",
      "                                                                 \n",
      " batch_normalization_16 (Bat  (None, 14, 14, 128)      512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " up_sampling2d_5 (UpSampling  (None, 28, 28, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_19 (Conv2D)          (None, 28, 28, 64)        73792     \n",
      "                                                                 \n",
      " activation_7 (Activation)   (None, 28, 28, 64)        0         \n",
      "                                                                 \n",
      " batch_normalization_17 (Bat  (None, 28, 28, 64)       256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv2d_20 (Conv2D)          (None, 28, 28, 1)         577       \n",
      "                                                                 \n",
      " activation_8 (Activation)   (None, 28, 28, 1)         0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 681,089\n",
      "Trainable params: 680,449\n",
      "Non-trainable params: 640\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:5 out of the last 7 calls to <function Model.make_train_function.<locals>.train_function at 0x7fe037b02440> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 9 calls to <function Model.make_train_function.<locals>.train_function at 0x7fe037afc830> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "0 [D loss: 1.15, acc.: 38.28%] [Q loss: 0.61] [G loss: 2.30]\n",
      "1 [D loss: 2.43, acc.: 50.00%] [Q loss: 0.58] [G loss: 2.31]\n",
      "2 [D loss: 0.37, acc.: 80.47%] [Q loss: 0.62] [G loss: 2.28]\n",
      "3 [D loss: 0.44, acc.: 78.52%] [Q loss: 0.59] [G loss: 2.43]\n",
      "4 [D loss: 0.23, acc.: 92.58%] [Q loss: 0.55] [G loss: 2.34]\n",
      "5 [D loss: 0.17, acc.: 95.31%] [Q loss: 0.52] [G loss: 2.36]\n",
      "6 [D loss: 0.08, acc.: 98.44%] [Q loss: 0.50] [G loss: 2.40]\n",
      "7 [D loss: 0.07, acc.: 98.44%] [Q loss: 0.42] [G loss: 2.46]\n",
      "8 [D loss: 0.06, acc.: 98.83%] [Q loss: 0.36] [G loss: 2.46]\n",
      "9 [D loss: 0.04, acc.: 99.22%] [Q loss: 0.24] [G loss: 2.50]\n",
      "10 [D loss: 0.03, acc.: 100.00%] [Q loss: 0.22] [G loss: 2.48]\n",
      "11 [D loss: 0.03, acc.: 99.22%] [Q loss: 0.17] [G loss: 2.55]\n",
      "12 [D loss: 0.02, acc.: 100.00%] [Q loss: 0.12] [G loss: 2.43]\n",
      "13 [D loss: 0.03, acc.: 99.22%] [Q loss: 0.07] [G loss: 2.71]\n",
      "14 [D loss: 0.03, acc.: 99.61%] [Q loss: 0.08] [G loss: 2.75]\n",
      "15 [D loss: 0.01, acc.: 100.00%] [Q loss: 0.06] [G loss: 2.79]\n",
      "16 [D loss: 0.01, acc.: 100.00%] [Q loss: 0.05] [G loss: 2.84]\n",
      "17 [D loss: 0.01, acc.: 100.00%] [Q loss: 0.04] [G loss: 2.90]\n",
      "18 [D loss: 0.02, acc.: 99.22%] [Q loss: 0.04] [G loss: 3.11]\n",
      "19 [D loss: 0.06, acc.: 98.05%] [Q loss: 0.22] [G loss: 2.71]\n",
      "20 [D loss: 0.11, acc.: 96.09%] [Q loss: 0.80] [G loss: 2.62]\n",
      "21 [D loss: 0.28, acc.: 87.89%] [Q loss: 5.31] [G loss: 2.58]\n",
      "22 [D loss: 2.74, acc.: 23.83%] [Q loss: 8.49] [G loss: 2.61]\n",
      "23 [D loss: 1.10, acc.: 46.09%] [Q loss: 8.91] [G loss: 2.38]\n",
      "24 [D loss: 3.10, acc.: 3.52%] [Q loss: 9.45] [G loss: 2.46]\n",
      "25 [D loss: 1.44, acc.: 35.55%] [Q loss: 8.10] [G loss: 2.48]\n",
      "26 [D loss: 1.26, acc.: 49.61%] [Q loss: 4.08] [G loss: 2.41]\n",
      "27 [D loss: 0.91, acc.: 55.08%] [Q loss: 2.96] [G loss: 2.41]\n",
      "28 [D loss: 0.58, acc.: 75.00%] [Q loss: 2.44] [G loss: 2.51]\n",
      "29 [D loss: 0.40, acc.: 83.98%] [Q loss: 2.14] [G loss: 2.42]\n",
      "30 [D loss: 0.29, acc.: 89.06%] [Q loss: 1.63] [G loss: 2.65]\n",
      "31 [D loss: 0.22, acc.: 90.62%] [Q loss: 1.86] [G loss: 2.43]\n",
      "32 [D loss: 0.27, acc.: 89.84%] [Q loss: 1.61] [G loss: 2.51]\n",
      "33 [D loss: 0.39, acc.: 81.25%] [Q loss: 0.83] [G loss: 2.42]\n",
      "34 [D loss: 0.19, acc.: 93.36%] [Q loss: 0.75] [G loss: 2.59]\n",
      "35 [D loss: 0.14, acc.: 95.31%] [Q loss: 1.08] [G loss: 2.47]\n",
      "36 [D loss: 0.16, acc.: 94.92%] [Q loss: 1.27] [G loss: 2.42]\n",
      "37 [D loss: 0.08, acc.: 98.44%] [Q loss: 1.48] [G loss: 2.53]\n",
      "38 [D loss: 0.10, acc.: 97.66%] [Q loss: 1.61] [G loss: 2.40]\n",
      "39 [D loss: 0.06, acc.: 99.22%] [Q loss: 2.01] [G loss: 2.56]\n",
      "40 [D loss: 0.04, acc.: 99.61%] [Q loss: 1.85] [G loss: 2.48]\n",
      "41 [D loss: 0.05, acc.: 99.22%] [Q loss: 1.47] [G loss: 2.49]\n",
      "42 [D loss: 0.03, acc.: 99.61%] [Q loss: 1.45] [G loss: 2.41]\n",
      "43 [D loss: 0.06, acc.: 99.61%] [Q loss: 1.71] [G loss: 2.43]\n",
      "44 [D loss: 0.04, acc.: 99.61%] [Q loss: 1.53] [G loss: 2.44]\n",
      "45 [D loss: 0.05, acc.: 99.22%] [Q loss: 1.51] [G loss: 2.45]\n",
      "46 [D loss: 0.05, acc.: 99.61%] [Q loss: 1.57] [G loss: 2.43]\n",
      "47 [D loss: 0.03, acc.: 100.00%] [Q loss: 1.58] [G loss: 2.33]\n",
      "48 [D loss: 0.02, acc.: 100.00%] [Q loss: 1.34] [G loss: 2.51]\n",
      "49 [D loss: 0.01, acc.: 100.00%] [Q loss: 1.45] [G loss: 2.25]\n",
      "50 [D loss: 0.01, acc.: 100.00%] [Q loss: 1.51] [G loss: 2.33]\n",
      "51 [D loss: 0.03, acc.: 100.00%] [Q loss: 1.54] [G loss: 2.29]\n",
      "52 [D loss: 0.02, acc.: 100.00%] [Q loss: 1.54] [G loss: 2.42]\n",
      "53 [D loss: 0.02, acc.: 100.00%] [Q loss: 1.50] [G loss: 2.29]\n",
      "54 [D loss: 0.02, acc.: 99.61%] [Q loss: 1.56] [G loss: 2.28]\n",
      "55 [D loss: 0.02, acc.: 100.00%] [Q loss: 1.67] [G loss: 2.32]\n",
      "56 [D loss: 0.02, acc.: 100.00%] [Q loss: 1.36] [G loss: 2.44]\n",
      "57 [D loss: 0.01, acc.: 100.00%] [Q loss: 1.37] [G loss: 2.29]\n",
      "58 [D loss: 0.01, acc.: 100.00%] [Q loss: 1.27] [G loss: 2.31]\n",
      "59 [D loss: 0.02, acc.: 100.00%] [Q loss: 1.38] [G loss: 2.31]\n",
      "60 [D loss: 0.01, acc.: 100.00%] [Q loss: 1.39] [G loss: 2.28]\n",
      "61 [D loss: 0.01, acc.: 100.00%] [Q loss: 1.30] [G loss: 2.21]\n",
      "62 [D loss: 0.01, acc.: 100.00%] [Q loss: 1.45] [G loss: 2.35]\n",
      "63 [D loss: 0.01, acc.: 100.00%] [Q loss: 1.40] [G loss: 2.39]\n",
      "64 [D loss: 0.01, acc.: 100.00%] [Q loss: 1.52] [G loss: 2.39]\n",
      "65 [D loss: 0.01, acc.: 100.00%] [Q loss: 1.36] [G loss: 2.30]\n",
      "66 [D loss: 0.01, acc.: 100.00%] [Q loss: 1.23] [G loss: 2.23]\n",
      "67 [D loss: 0.01, acc.: 100.00%] [Q loss: 1.25] [G loss: 2.28]\n",
      "68 [D loss: 0.01, acc.: 100.00%] [Q loss: 1.23] [G loss: 2.30]\n",
      "69 [D loss: 0.01, acc.: 100.00%] [Q loss: 1.28] [G loss: 2.31]\n",
      "70 [D loss: 0.01, acc.: 100.00%] [Q loss: 1.22] [G loss: 2.25]\n",
      "71 [D loss: 0.01, acc.: 100.00%] [Q loss: 1.41] [G loss: 2.39]\n",
      "72 [D loss: 0.01, acc.: 100.00%] [Q loss: 1.36] [G loss: 2.35]\n",
      "73 [D loss: 0.01, acc.: 100.00%] [Q loss: 1.17] [G loss: 2.23]\n",
      "74 [D loss: 0.01, acc.: 100.00%] [Q loss: 1.13] [G loss: 2.24]\n",
      "75 [D loss: 0.01, acc.: 100.00%] [Q loss: 1.23] [G loss: 2.22]\n",
      "76 [D loss: 0.01, acc.: 100.00%] [Q loss: 1.50] [G loss: 2.29]\n",
      "77 [D loss: 0.01, acc.: 100.00%] [Q loss: 1.34] [G loss: 2.27]\n",
      "78 [D loss: 0.01, acc.: 100.00%] [Q loss: 1.26] [G loss: 2.27]\n",
      "79 [D loss: 0.01, acc.: 100.00%] [Q loss: 1.22] [G loss: 2.20]\n",
      "80 [D loss: 0.01, acc.: 100.00%] [Q loss: 1.29] [G loss: 2.29]\n",
      "81 [D loss: 0.01, acc.: 100.00%] [Q loss: 1.18] [G loss: 2.30]\n",
      "82 [D loss: 0.01, acc.: 100.00%] [Q loss: 1.18] [G loss: 2.23]\n",
      "83 [D loss: 0.01, acc.: 100.00%] [Q loss: 1.17] [G loss: 2.18]\n",
      "84 [D loss: 0.01, acc.: 100.00%] [Q loss: 1.33] [G loss: 2.16]\n",
      "85 [D loss: 0.01, acc.: 100.00%] [Q loss: 1.17] [G loss: 2.21]\n",
      "86 [D loss: 0.01, acc.: 100.00%] [Q loss: 1.14] [G loss: 2.13]\n",
      "87 [D loss: 0.01, acc.: 100.00%] [Q loss: 1.11] [G loss: 2.14]\n",
      "88 [D loss: 0.01, acc.: 100.00%] [Q loss: 1.15] [G loss: 2.28]\n",
      "89 [D loss: 0.01, acc.: 100.00%] [Q loss: 1.26] [G loss: 2.14]\n",
      "90 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.20] [G loss: 2.24]\n",
      "91 [D loss: 0.01, acc.: 100.00%] [Q loss: 1.11] [G loss: 2.12]\n",
      "92 [D loss: 0.01, acc.: 100.00%] [Q loss: 1.08] [G loss: 2.15]\n",
      "93 [D loss: 0.01, acc.: 100.00%] [Q loss: 1.24] [G loss: 2.29]\n",
      "94 [D loss: 0.01, acc.: 100.00%] [Q loss: 1.08] [G loss: 2.22]\n",
      "95 [D loss: 0.01, acc.: 100.00%] [Q loss: 1.19] [G loss: 2.14]\n",
      "96 [D loss: 0.01, acc.: 100.00%] [Q loss: 1.38] [G loss: 2.09]\n",
      "97 [D loss: 0.01, acc.: 100.00%] [Q loss: 1.28] [G loss: 2.10]\n",
      "98 [D loss: 0.01, acc.: 100.00%] [Q loss: 1.23] [G loss: 2.12]\n",
      "99 [D loss: 0.01, acc.: 100.00%] [Q loss: 1.19] [G loss: 2.17]\n",
      "100 [D loss: 0.01, acc.: 100.00%] [Q loss: 1.16] [G loss: 2.05]\n",
      "101 [D loss: 0.01, acc.: 100.00%] [Q loss: 1.23] [G loss: 2.10]\n",
      "102 [D loss: 0.01, acc.: 100.00%] [Q loss: 1.26] [G loss: 2.06]\n",
      "103 [D loss: 0.01, acc.: 100.00%] [Q loss: 1.32] [G loss: 2.11]\n",
      "104 [D loss: 0.01, acc.: 100.00%] [Q loss: 1.03] [G loss: 2.08]\n",
      "105 [D loss: 0.01, acc.: 100.00%] [Q loss: 1.19] [G loss: 1.97]\n",
      "106 [D loss: 0.01, acc.: 100.00%] [Q loss: 1.13] [G loss: 2.02]\n",
      "107 [D loss: 0.01, acc.: 100.00%] [Q loss: 1.17] [G loss: 2.12]\n",
      "108 [D loss: 0.01, acc.: 100.00%] [Q loss: 1.18] [G loss: 2.07]\n",
      "109 [D loss: 0.01, acc.: 100.00%] [Q loss: 1.08] [G loss: 2.03]\n",
      "110 [D loss: 0.02, acc.: 100.00%] [Q loss: 1.33] [G loss: 2.11]\n",
      "111 [D loss: 0.01, acc.: 99.61%] [Q loss: 1.25] [G loss: 1.96]\n",
      "112 [D loss: 0.01, acc.: 100.00%] [Q loss: 1.27] [G loss: 2.11]\n",
      "113 [D loss: 0.01, acc.: 100.00%] [Q loss: 1.19] [G loss: 1.93]\n",
      "114 [D loss: 0.02, acc.: 99.61%] [Q loss: 0.97] [G loss: 1.95]\n",
      "115 [D loss: 0.01, acc.: 100.00%] [Q loss: 1.09] [G loss: 1.88]\n",
      "116 [D loss: 0.01, acc.: 100.00%] [Q loss: 1.07] [G loss: 2.04]\n",
      "117 [D loss: 0.01, acc.: 100.00%] [Q loss: 1.19] [G loss: 1.90]\n",
      "118 [D loss: 0.01, acc.: 100.00%] [Q loss: 1.20] [G loss: 1.95]\n",
      "119 [D loss: 0.01, acc.: 100.00%] [Q loss: 1.17] [G loss: 1.88]\n",
      "120 [D loss: 0.01, acc.: 100.00%] [Q loss: 1.25] [G loss: 1.80]\n",
      "121 [D loss: 0.01, acc.: 100.00%] [Q loss: 1.11] [G loss: 1.84]\n",
      "122 [D loss: 0.01, acc.: 100.00%] [Q loss: 1.17] [G loss: 1.72]\n",
      "123 [D loss: 0.02, acc.: 100.00%] [Q loss: 1.21] [G loss: 1.78]\n",
      "124 [D loss: 0.01, acc.: 100.00%] [Q loss: 1.32] [G loss: 1.70]\n",
      "125 [D loss: 0.02, acc.: 100.00%] [Q loss: 1.39] [G loss: 1.82]\n",
      "126 [D loss: 0.01, acc.: 100.00%] [Q loss: 1.45] [G loss: 1.74]\n",
      "127 [D loss: 0.03, acc.: 100.00%] [Q loss: 1.07] [G loss: 1.79]\n",
      "128 [D loss: 0.01, acc.: 100.00%] [Q loss: 1.19] [G loss: 1.69]\n",
      "129 [D loss: 0.01, acc.: 100.00%] [Q loss: 1.17] [G loss: 1.65]\n",
      "130 [D loss: 0.01, acc.: 100.00%] [Q loss: 1.09] [G loss: 1.63]\n",
      "131 [D loss: 0.01, acc.: 100.00%] [Q loss: 1.21] [G loss: 1.60]\n",
      "132 [D loss: 0.01, acc.: 100.00%] [Q loss: 1.21] [G loss: 1.47]\n",
      "133 [D loss: 0.01, acc.: 100.00%] [Q loss: 1.12] [G loss: 1.36]\n",
      "134 [D loss: 0.01, acc.: 100.00%] [Q loss: 1.00] [G loss: 1.53]\n",
      "135 [D loss: 0.01, acc.: 100.00%] [Q loss: 1.13] [G loss: 1.42]\n",
      "136 [D loss: 0.01, acc.: 100.00%] [Q loss: 1.17] [G loss: 1.37]\n",
      "137 [D loss: 0.01, acc.: 100.00%] [Q loss: 1.12] [G loss: 1.40]\n",
      "138 [D loss: 0.01, acc.: 100.00%] [Q loss: 1.17] [G loss: 1.37]\n",
      "139 [D loss: 0.01, acc.: 100.00%] [Q loss: 1.20] [G loss: 1.23]\n",
      "140 [D loss: 0.01, acc.: 100.00%] [Q loss: 1.05] [G loss: 1.21]\n",
      "141 [D loss: 0.01, acc.: 100.00%] [Q loss: 1.10] [G loss: 1.18]\n",
      "142 [D loss: 0.01, acc.: 100.00%] [Q loss: 1.08] [G loss: 1.09]\n",
      "143 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.07] [G loss: 1.11]\n",
      "144 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.33] [G loss: 1.12]\n",
      "145 [D loss: 0.01, acc.: 100.00%] [Q loss: 1.06] [G loss: 1.15]\n",
      "146 [D loss: 0.01, acc.: 100.00%] [Q loss: 0.93] [G loss: 1.08]\n",
      "147 [D loss: 0.01, acc.: 100.00%] [Q loss: 1.12] [G loss: 0.94]\n",
      "148 [D loss: 0.01, acc.: 99.61%] [Q loss: 1.26] [G loss: 0.93]\n",
      "149 [D loss: 0.01, acc.: 100.00%] [Q loss: 1.18] [G loss: 0.90]\n",
      "150 [D loss: 0.01, acc.: 100.00%] [Q loss: 1.22] [G loss: 0.92]\n",
      "151 [D loss: 0.01, acc.: 100.00%] [Q loss: 1.08] [G loss: 0.83]\n",
      "152 [D loss: 0.01, acc.: 100.00%] [Q loss: 1.01] [G loss: 0.88]\n",
      "153 [D loss: 0.01, acc.: 100.00%] [Q loss: 1.01] [G loss: 0.77]\n",
      "154 [D loss: 0.01, acc.: 100.00%] [Q loss: 0.94] [G loss: 0.79]\n",
      "155 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.59] [G loss: 0.67]\n",
      "156 [D loss: 0.01, acc.: 100.00%] [Q loss: 1.06] [G loss: 0.73]\n",
      "157 [D loss: 0.02, acc.: 100.00%] [Q loss: 0.95] [G loss: 0.71]\n",
      "158 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.10] [G loss: 0.77]\n",
      "159 [D loss: 0.01, acc.: 100.00%] [Q loss: 1.03] [G loss: 0.59]\n",
      "160 [D loss: 0.01, acc.: 100.00%] [Q loss: 1.10] [G loss: 0.62]\n",
      "161 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.30] [G loss: 0.55]\n",
      "162 [D loss: 0.01, acc.: 100.00%] [Q loss: 1.07] [G loss: 0.59]\n",
      "163 [D loss: 0.01, acc.: 100.00%] [Q loss: 0.93] [G loss: 0.54]\n",
      "164 [D loss: 0.00, acc.: 100.00%] [Q loss: 0.89] [G loss: 0.51]\n",
      "165 [D loss: 0.01, acc.: 100.00%] [Q loss: 0.93] [G loss: 0.51]\n",
      "166 [D loss: 0.00, acc.: 100.00%] [Q loss: 0.86] [G loss: 0.59]\n",
      "167 [D loss: 0.01, acc.: 100.00%] [Q loss: 0.95] [G loss: 0.44]\n",
      "168 [D loss: 0.01, acc.: 100.00%] [Q loss: 1.10] [G loss: 0.37]\n",
      "169 [D loss: 0.01, acc.: 100.00%] [Q loss: 0.90] [G loss: 0.51]\n",
      "170 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.23] [G loss: 0.45]\n",
      "171 [D loss: 0.00, acc.: 100.00%] [Q loss: 0.95] [G loss: 0.48]\n",
      "172 [D loss: 0.01, acc.: 100.00%] [Q loss: 1.04] [G loss: 0.46]\n",
      "173 [D loss: 0.01, acc.: 100.00%] [Q loss: 1.04] [G loss: 0.40]\n",
      "174 [D loss: 0.00, acc.: 100.00%] [Q loss: 0.99] [G loss: 0.44]\n",
      "175 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.08] [G loss: 0.45]\n",
      "176 [D loss: 0.03, acc.: 100.00%] [Q loss: 1.13] [G loss: 0.52]\n",
      "177 [D loss: 0.01, acc.: 100.00%] [Q loss: 1.16] [G loss: 0.41]\n",
      "178 [D loss: 0.02, acc.: 100.00%] [Q loss: 0.81] [G loss: 0.36]\n",
      "179 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.03] [G loss: 0.39]\n",
      "180 [D loss: 0.00, acc.: 100.00%] [Q loss: 0.90] [G loss: 0.40]\n",
      "181 [D loss: 0.01, acc.: 100.00%] [Q loss: 1.00] [G loss: 0.29]\n",
      "182 [D loss: 0.00, acc.: 100.00%] [Q loss: 0.97] [G loss: 0.24]\n",
      "183 [D loss: 0.01, acc.: 100.00%] [Q loss: 0.70] [G loss: 0.39]\n",
      "184 [D loss: 0.01, acc.: 100.00%] [Q loss: 0.80] [G loss: 0.33]\n",
      "185 [D loss: 0.01, acc.: 100.00%] [Q loss: 0.95] [G loss: 0.24]\n",
      "186 [D loss: 0.00, acc.: 100.00%] [Q loss: 0.89] [G loss: 0.43]\n",
      "187 [D loss: 0.01, acc.: 100.00%] [Q loss: 0.86] [G loss: 0.23]\n",
      "188 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.16] [G loss: 0.34]\n",
      "189 [D loss: 0.01, acc.: 100.00%] [Q loss: 1.05] [G loss: 0.28]\n",
      "190 [D loss: 0.01, acc.: 100.00%] [Q loss: 1.02] [G loss: 0.24]\n",
      "191 [D loss: 0.01, acc.: 100.00%] [Q loss: 1.07] [G loss: 0.22]\n",
      "192 [D loss: 0.01, acc.: 100.00%] [Q loss: 0.66] [G loss: 0.29]\n",
      "193 [D loss: 0.00, acc.: 100.00%] [Q loss: 0.83] [G loss: 0.25]\n",
      "194 [D loss: 0.01, acc.: 100.00%] [Q loss: 0.83] [G loss: 0.28]\n",
      "195 [D loss: 0.01, acc.: 100.00%] [Q loss: 0.78] [G loss: 0.23]\n",
      "196 [D loss: 0.00, acc.: 100.00%] [Q loss: 0.68] [G loss: 0.20]\n",
      "197 [D loss: 0.00, acc.: 100.00%] [Q loss: 0.73] [G loss: 0.22]\n",
      "198 [D loss: 0.00, acc.: 100.00%] [Q loss: 0.90] [G loss: 0.16]\n",
      "199 [D loss: 0.00, acc.: 100.00%] [Q loss: 0.77] [G loss: 0.21]\n",
      "200 [D loss: 0.00, acc.: 100.00%] [Q loss: 0.68] [G loss: 0.23]\n",
      "201 [D loss: 0.01, acc.: 100.00%] [Q loss: 1.06] [G loss: 0.26]\n",
      "202 [D loss: 0.01, acc.: 100.00%] [Q loss: 0.96] [G loss: 0.19]\n",
      "203 [D loss: 0.01, acc.: 100.00%] [Q loss: 0.66] [G loss: 0.24]\n",
      "204 [D loss: 0.00, acc.: 100.00%] [Q loss: 0.79] [G loss: 0.19]\n",
      "205 [D loss: 0.01, acc.: 100.00%] [Q loss: 0.69] [G loss: 0.24]\n",
      "206 [D loss: 0.00, acc.: 100.00%] [Q loss: 0.68] [G loss: 0.25]\n",
      "207 [D loss: 0.00, acc.: 100.00%] [Q loss: 0.73] [G loss: 0.20]\n",
      "208 [D loss: 0.03, acc.: 99.22%] [Q loss: 1.15] [G loss: 0.15]\n",
      "209 [D loss: 0.03, acc.: 99.61%] [Q loss: 0.69] [G loss: 0.26]\n",
      "210 [D loss: 0.00, acc.: 100.00%] [Q loss: 0.77] [G loss: 0.13]\n",
      "211 [D loss: 0.01, acc.: 100.00%] [Q loss: 0.70] [G loss: 0.27]\n",
      "212 [D loss: 0.00, acc.: 100.00%] [Q loss: 0.58] [G loss: 0.15]\n",
      "213 [D loss: 0.00, acc.: 100.00%] [Q loss: 0.66] [G loss: 0.14]\n",
      "214 [D loss: 0.00, acc.: 100.00%] [Q loss: 0.55] [G loss: 0.20]\n",
      "215 [D loss: 0.00, acc.: 100.00%] [Q loss: 0.80] [G loss: 0.15]\n",
      "216 [D loss: 0.00, acc.: 100.00%] [Q loss: 0.74] [G loss: 0.22]\n",
      "217 [D loss: 0.00, acc.: 100.00%] [Q loss: 0.73] [G loss: 0.13]\n",
      "218 [D loss: 0.00, acc.: 100.00%] [Q loss: 0.61] [G loss: 0.17]\n",
      "219 [D loss: 0.00, acc.: 100.00%] [Q loss: 0.66] [G loss: 0.17]\n",
      "220 [D loss: 0.00, acc.: 100.00%] [Q loss: 0.65] [G loss: 0.15]\n",
      "221 [D loss: 0.00, acc.: 100.00%] [Q loss: 0.62] [G loss: 0.12]\n",
      "222 [D loss: 0.00, acc.: 100.00%] [Q loss: 0.61] [G loss: 0.18]\n",
      "223 [D loss: 0.00, acc.: 100.00%] [Q loss: 0.57] [G loss: 0.25]\n",
      "224 [D loss: 0.01, acc.: 99.61%] [Q loss: 0.47] [G loss: 0.16]\n",
      "225 [D loss: 0.01, acc.: 100.00%] [Q loss: 0.50] [G loss: 0.10]\n",
      "226 [D loss: 0.00, acc.: 100.00%] [Q loss: 0.70] [G loss: 0.17]\n",
      "227 [D loss: 0.00, acc.: 100.00%] [Q loss: 0.65] [G loss: 0.11]\n",
      "228 [D loss: 0.00, acc.: 100.00%] [Q loss: 0.57] [G loss: 0.14]\n",
      "229 [D loss: 0.00, acc.: 100.00%] [Q loss: 0.64] [G loss: 0.09]\n",
      "230 [D loss: 0.00, acc.: 100.00%] [Q loss: 0.74] [G loss: 0.17]\n",
      "231 [D loss: 0.00, acc.: 100.00%] [Q loss: 0.61] [G loss: 0.15]\n",
      "232 [D loss: 0.01, acc.: 100.00%] [Q loss: 0.81] [G loss: 0.22]\n",
      "233 [D loss: 0.00, acc.: 100.00%] [Q loss: 0.66] [G loss: 0.11]\n",
      "234 [D loss: 0.00, acc.: 100.00%] [Q loss: 0.60] [G loss: 0.15]\n",
      "235 [D loss: 0.01, acc.: 100.00%] [Q loss: 0.50] [G loss: 0.17]\n",
      "236 [D loss: 0.00, acc.: 100.00%] [Q loss: 0.51] [G loss: 0.16]\n",
      "237 [D loss: 0.00, acc.: 100.00%] [Q loss: 0.60] [G loss: 0.17]\n",
      "238 [D loss: 0.00, acc.: 100.00%] [Q loss: 0.68] [G loss: 0.09]\n",
      "239 [D loss: 0.02, acc.: 99.61%] [Q loss: 0.90] [G loss: 0.16]\n",
      "240 [D loss: 0.01, acc.: 100.00%] [Q loss: 0.79] [G loss: 0.20]\n",
      "241 [D loss: 0.00, acc.: 100.00%] [Q loss: 0.60] [G loss: 0.17]\n",
      "242 [D loss: 0.00, acc.: 100.00%] [Q loss: 0.65] [G loss: 0.12]\n",
      "243 [D loss: 0.00, acc.: 100.00%] [Q loss: 0.73] [G loss: 0.10]\n",
      "244 [D loss: 0.00, acc.: 100.00%] [Q loss: 0.66] [G loss: 0.10]\n",
      "245 [D loss: 0.00, acc.: 100.00%] [Q loss: 0.61] [G loss: 0.20]\n",
      "246 [D loss: 0.00, acc.: 100.00%] [Q loss: 0.57] [G loss: 0.10]\n",
      "247 [D loss: 0.00, acc.: 100.00%] [Q loss: 0.56] [G loss: 0.12]\n",
      "248 [D loss: 0.00, acc.: 100.00%] [Q loss: 0.52] [G loss: 0.16]\n",
      "249 [D loss: 0.00, acc.: 100.00%] [Q loss: 0.63] [G loss: 0.12]\n",
      "250 [D loss: 0.01, acc.: 100.00%] [Q loss: 0.78] [G loss: 0.17]\n",
      "251 [D loss: 0.01, acc.: 100.00%] [Q loss: 0.44] [G loss: 0.21]\n",
      "252 [D loss: 0.01, acc.: 100.00%] [Q loss: 0.51] [G loss: 0.11]\n",
      "253 [D loss: 0.00, acc.: 100.00%] [Q loss: 0.93] [G loss: 0.12]\n",
      "254 [D loss: 0.00, acc.: 100.00%] [Q loss: 0.88] [G loss: 0.09]\n",
      "255 [D loss: 0.01, acc.: 100.00%] [Q loss: 0.91] [G loss: 0.19]\n",
      "256 [D loss: 0.01, acc.: 100.00%] [Q loss: 0.79] [G loss: 0.09]\n",
      "257 [D loss: 0.00, acc.: 100.00%] [Q loss: 0.76] [G loss: 0.10]\n",
      "258 [D loss: 0.00, acc.: 100.00%] [Q loss: 0.70] [G loss: 0.21]\n",
      "259 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.18] [G loss: 0.16]\n",
      "260 [D loss: 0.00, acc.: 100.00%] [Q loss: 0.78] [G loss: 0.16]\n",
      "261 [D loss: 0.02, acc.: 100.00%] [Q loss: 2.95] [G loss: 1.24]\n",
      "262 [D loss: 13.85, acc.: 0.00%] [Q loss: 0.22] [G loss: 8.46]\n",
      "263 [D loss: 3.31, acc.: 54.69%] [Q loss: 8.99] [G loss: 1.38]\n",
      "264 [D loss: 2.25, acc.: 61.33%] [Q loss: 6.02] [G loss: 5.02]\n",
      "265 [D loss: 0.76, acc.: 66.41%] [Q loss: 8.91] [G loss: 4.22]\n",
      "266 [D loss: 1.26, acc.: 64.06%] [Q loss: 4.20] [G loss: 1.46]\n",
      "267 [D loss: 0.36, acc.: 88.67%] [Q loss: 2.06] [G loss: 1.57]\n",
      "268 [D loss: 0.61, acc.: 71.88%] [Q loss: 0.13] [G loss: 2.54]\n",
      "269 [D loss: 0.10, acc.: 97.27%] [Q loss: 0.39] [G loss: 0.94]\n",
      "270 [D loss: 0.04, acc.: 100.00%] [Q loss: 0.69] [G loss: 0.75]\n",
      "271 [D loss: 0.10, acc.: 95.70%] [Q loss: 0.27] [G loss: 0.81]\n",
      "272 [D loss: 0.06, acc.: 98.44%] [Q loss: 0.38] [G loss: 0.58]\n",
      "273 [D loss: 0.08, acc.: 97.27%] [Q loss: 0.32] [G loss: 0.63]\n",
      "274 [D loss: 0.07, acc.: 99.22%] [Q loss: 0.32] [G loss: 0.61]\n",
      "275 [D loss: 0.09, acc.: 97.66%] [Q loss: 0.36] [G loss: 0.47]\n",
      "276 [D loss: 0.06, acc.: 99.61%] [Q loss: 0.39] [G loss: 0.55]\n",
      "277 [D loss: 0.15, acc.: 97.27%] [Q loss: 0.44] [G loss: 0.40]\n",
      "278 [D loss: 0.11, acc.: 96.88%] [Q loss: 0.50] [G loss: 0.53]\n",
      "279 [D loss: 0.07, acc.: 99.61%] [Q loss: 0.57] [G loss: 0.36]\n",
      "280 [D loss: 0.15, acc.: 95.31%] [Q loss: 0.72] [G loss: 0.50]\n",
      "281 [D loss: 0.37, acc.: 85.16%] [Q loss: 1.36] [G loss: 0.54]\n",
      "282 [D loss: 0.19, acc.: 94.92%] [Q loss: 1.11] [G loss: 0.52]\n",
      "283 [D loss: 0.04, acc.: 100.00%] [Q loss: 1.42] [G loss: 0.37]\n",
      "284 [D loss: 0.10, acc.: 97.66%] [Q loss: 1.40] [G loss: 0.49]\n",
      "285 [D loss: 0.14, acc.: 96.88%] [Q loss: 1.68] [G loss: 0.56]\n",
      "286 [D loss: 0.05, acc.: 99.61%] [Q loss: 2.06] [G loss: 0.47]\n",
      "287 [D loss: 0.15, acc.: 96.88%] [Q loss: 1.98] [G loss: 0.63]\n",
      "288 [D loss: 0.17, acc.: 95.70%] [Q loss: 1.29] [G loss: 0.34]\n",
      "289 [D loss: 0.10, acc.: 98.83%] [Q loss: 1.66] [G loss: 0.36]\n",
      "290 [D loss: 0.05, acc.: 100.00%] [Q loss: 1.78] [G loss: 0.35]\n",
      "291 [D loss: 0.17, acc.: 96.09%] [Q loss: 1.38] [G loss: 0.43]\n",
      "292 [D loss: 0.10, acc.: 98.05%] [Q loss: 1.03] [G loss: 0.28]\n",
      "293 [D loss: 0.06, acc.: 99.61%] [Q loss: 1.29] [G loss: 0.26]\n",
      "294 [D loss: 0.03, acc.: 100.00%] [Q loss: 1.20] [G loss: 0.34]\n",
      "295 [D loss: 0.03, acc.: 100.00%] [Q loss: 1.29] [G loss: 0.22]\n",
      "296 [D loss: 0.07, acc.: 99.61%] [Q loss: 1.11] [G loss: 0.19]\n",
      "297 [D loss: 0.06, acc.: 98.83%] [Q loss: 1.43] [G loss: 0.21]\n",
      "298 [D loss: 0.05, acc.: 100.00%] [Q loss: 1.16] [G loss: 0.40]\n",
      "299 [D loss: 0.06, acc.: 100.00%] [Q loss: 1.21] [G loss: 0.15]\n",
      "300 [D loss: 0.02, acc.: 100.00%] [Q loss: 0.97] [G loss: 0.14]\n",
      "301 [D loss: 0.09, acc.: 97.66%] [Q loss: 1.30] [G loss: 0.28]\n",
      "302 [D loss: 0.06, acc.: 99.61%] [Q loss: 1.13] [G loss: 0.42]\n",
      "303 [D loss: 0.06, acc.: 98.83%] [Q loss: 1.05] [G loss: 0.12]\n",
      "304 [D loss: 0.02, acc.: 100.00%] [Q loss: 1.14] [G loss: 0.17]\n",
      "305 [D loss: 0.06, acc.: 99.61%] [Q loss: 1.15] [G loss: 0.11]\n",
      "306 [D loss: 0.05, acc.: 100.00%] [Q loss: 1.03] [G loss: 0.56]\n",
      "307 [D loss: 0.01, acc.: 100.00%] [Q loss: 1.60] [G loss: 0.08]\n",
      "308 [D loss: 0.01, acc.: 100.00%] [Q loss: 1.74] [G loss: 0.21]\n",
      "309 [D loss: 0.03, acc.: 100.00%] [Q loss: 1.36] [G loss: 0.22]\n",
      "310 [D loss: 0.02, acc.: 100.00%] [Q loss: 1.01] [G loss: 0.79]\n",
      "311 [D loss: 0.02, acc.: 100.00%] [Q loss: 1.57] [G loss: 0.12]\n",
      "312 [D loss: 0.02, acc.: 100.00%] [Q loss: 1.42] [G loss: 0.10]\n",
      "313 [D loss: 0.02, acc.: 100.00%] [Q loss: 1.26] [G loss: 0.06]\n",
      "314 [D loss: 0.02, acc.: 100.00%] [Q loss: 1.22] [G loss: 0.14]\n",
      "315 [D loss: 0.02, acc.: 100.00%] [Q loss: 1.20] [G loss: 0.17]\n",
      "316 [D loss: 0.02, acc.: 100.00%] [Q loss: 1.15] [G loss: 0.20]\n",
      "317 [D loss: 0.04, acc.: 100.00%] [Q loss: 0.84] [G loss: 0.12]\n",
      "318 [D loss: 0.01, acc.: 100.00%] [Q loss: 0.93] [G loss: 0.18]\n",
      "319 [D loss: 0.04, acc.: 99.61%] [Q loss: 0.93] [G loss: 0.12]\n",
      "320 [D loss: 0.01, acc.: 100.00%] [Q loss: 1.02] [G loss: 0.10]\n",
      "321 [D loss: 0.02, acc.: 100.00%] [Q loss: 0.82] [G loss: 0.19]\n",
      "322 [D loss: 0.03, acc.: 100.00%] [Q loss: 0.73] [G loss: 0.14]\n",
      "323 [D loss: 0.01, acc.: 100.00%] [Q loss: 0.91] [G loss: 0.14]\n",
      "324 [D loss: 0.02, acc.: 100.00%] [Q loss: 0.83] [G loss: 0.12]\n",
      "325 [D loss: 0.02, acc.: 100.00%] [Q loss: 0.76] [G loss: 0.26]\n",
      "326 [D loss: 0.01, acc.: 100.00%] [Q loss: 1.04] [G loss: 0.11]\n",
      "327 [D loss: 0.02, acc.: 100.00%] [Q loss: 0.76] [G loss: 0.07]\n",
      "328 [D loss: 0.02, acc.: 100.00%] [Q loss: 0.88] [G loss: 0.13]\n",
      "329 [D loss: 0.01, acc.: 100.00%] [Q loss: 0.82] [G loss: 0.19]\n",
      "330 [D loss: 0.02, acc.: 100.00%] [Q loss: 0.83] [G loss: 0.08]\n",
      "331 [D loss: 0.01, acc.: 100.00%] [Q loss: 0.72] [G loss: 0.10]\n",
      "332 [D loss: 0.01, acc.: 100.00%] [Q loss: 0.61] [G loss: 0.13]\n",
      "333 [D loss: 0.01, acc.: 100.00%] [Q loss: 0.78] [G loss: 0.19]\n",
      "334 [D loss: 0.02, acc.: 100.00%] [Q loss: 0.83] [G loss: 0.08]\n",
      "335 [D loss: 0.01, acc.: 100.00%] [Q loss: 0.80] [G loss: 0.11]\n",
      "336 [D loss: 0.01, acc.: 100.00%] [Q loss: 0.80] [G loss: 0.14]\n",
      "337 [D loss: 0.02, acc.: 100.00%] [Q loss: 0.85] [G loss: 0.12]\n",
      "338 [D loss: 0.05, acc.: 99.22%] [Q loss: 0.61] [G loss: 0.07]\n",
      "339 [D loss: 0.03, acc.: 100.00%] [Q loss: 1.03] [G loss: 0.10]\n",
      "340 [D loss: 0.01, acc.: 100.00%] [Q loss: 1.01] [G loss: 0.17]\n",
      "341 [D loss: 0.02, acc.: 100.00%] [Q loss: 0.59] [G loss: 0.12]\n",
      "342 [D loss: 0.01, acc.: 100.00%] [Q loss: 0.59] [G loss: 0.11]\n",
      "343 [D loss: 0.01, acc.: 100.00%] [Q loss: 0.65] [G loss: 0.12]\n",
      "344 [D loss: 0.01, acc.: 100.00%] [Q loss: 0.67] [G loss: 0.12]\n",
      "345 [D loss: 0.01, acc.: 100.00%] [Q loss: 0.53] [G loss: 0.14]\n",
      "346 [D loss: 0.01, acc.: 100.00%] [Q loss: 0.52] [G loss: 0.12]\n",
      "347 [D loss: 0.01, acc.: 100.00%] [Q loss: 0.73] [G loss: 0.06]\n",
      "348 [D loss: 0.01, acc.: 100.00%] [Q loss: 0.67] [G loss: 0.12]\n",
      "349 [D loss: 0.01, acc.: 100.00%] [Q loss: 0.56] [G loss: 0.10]\n",
      "350 [D loss: 0.00, acc.: 100.00%] [Q loss: 0.60] [G loss: 0.12]\n",
      "351 [D loss: 0.01, acc.: 100.00%] [Q loss: 0.56] [G loss: 0.08]\n",
      "352 [D loss: 0.01, acc.: 100.00%] [Q loss: 0.60] [G loss: 0.10]\n",
      "353 [D loss: 0.01, acc.: 100.00%] [Q loss: 0.57] [G loss: 0.07]\n",
      "354 [D loss: 0.00, acc.: 100.00%] [Q loss: 0.56] [G loss: 0.10]\n",
      "355 [D loss: 0.01, acc.: 100.00%] [Q loss: 0.44] [G loss: 0.07]\n",
      "356 [D loss: 0.01, acc.: 100.00%] [Q loss: 0.51] [G loss: 0.09]\n",
      "357 [D loss: 0.01, acc.: 100.00%] [Q loss: 0.72] [G loss: 0.11]\n",
      "358 [D loss: 0.01, acc.: 100.00%] [Q loss: 0.50] [G loss: 0.11]\n",
      "359 [D loss: 0.01, acc.: 100.00%] [Q loss: 0.55] [G loss: 0.08]\n",
      "360 [D loss: 0.01, acc.: 100.00%] [Q loss: 0.69] [G loss: 0.06]\n",
      "361 [D loss: 0.01, acc.: 100.00%] [Q loss: 0.68] [G loss: 0.10]\n",
      "362 [D loss: 0.01, acc.: 100.00%] [Q loss: 0.73] [G loss: 0.08]\n",
      "363 [D loss: 0.01, acc.: 100.00%] [Q loss: 0.71] [G loss: 0.12]\n",
      "364 [D loss: 0.00, acc.: 100.00%] [Q loss: 0.63] [G loss: 0.12]\n",
      "365 [D loss: 0.02, acc.: 99.61%] [Q loss: 0.83] [G loss: 0.10]\n",
      "366 [D loss: 0.01, acc.: 100.00%] [Q loss: 0.93] [G loss: 0.12]\n",
      "367 [D loss: 0.01, acc.: 100.00%] [Q loss: 0.62] [G loss: 0.11]\n",
      "368 [D loss: 0.01, acc.: 100.00%] [Q loss: 0.65] [G loss: 0.11]\n",
      "369 [D loss: 0.01, acc.: 100.00%] [Q loss: 0.66] [G loss: 0.10]\n",
      "370 [D loss: 0.01, acc.: 100.00%] [Q loss: 0.56] [G loss: 0.08]\n",
      "371 [D loss: 0.01, acc.: 100.00%] [Q loss: 0.55] [G loss: 0.07]\n",
      "372 [D loss: 0.00, acc.: 100.00%] [Q loss: 0.60] [G loss: 0.06]\n",
      "373 [D loss: 0.01, acc.: 100.00%] [Q loss: 0.63] [G loss: 0.17]\n",
      "374 [D loss: 0.00, acc.: 100.00%] [Q loss: 0.69] [G loss: 0.11]\n",
      "375 [D loss: 0.01, acc.: 100.00%] [Q loss: 0.67] [G loss: 0.11]\n",
      "376 [D loss: 0.01, acc.: 100.00%] [Q loss: 0.64] [G loss: 0.12]\n",
      "377 [D loss: 0.00, acc.: 100.00%] [Q loss: 0.74] [G loss: 0.06]\n",
      "378 [D loss: 0.01, acc.: 100.00%] [Q loss: 0.68] [G loss: 0.13]\n",
      "379 [D loss: 0.01, acc.: 100.00%] [Q loss: 0.68] [G loss: 0.09]\n",
      "380 [D loss: 0.01, acc.: 100.00%] [Q loss: 0.66] [G loss: 0.08]\n",
      "381 [D loss: 0.01, acc.: 100.00%] [Q loss: 0.84] [G loss: 0.14]\n",
      "382 [D loss: 0.03, acc.: 99.61%] [Q loss: 0.42] [G loss: 0.06]\n",
      "383 [D loss: 0.01, acc.: 100.00%] [Q loss: 0.62] [G loss: 0.07]\n",
      "384 [D loss: 0.01, acc.: 100.00%] [Q loss: 0.71] [G loss: 0.05]\n",
      "385 [D loss: 0.00, acc.: 100.00%] [Q loss: 0.74] [G loss: 0.07]\n",
      "386 [D loss: 0.01, acc.: 100.00%] [Q loss: 0.52] [G loss: 0.08]\n",
      "387 [D loss: 0.01, acc.: 100.00%] [Q loss: 0.61] [G loss: 0.07]\n",
      "388 [D loss: 0.01, acc.: 100.00%] [Q loss: 0.81] [G loss: 0.07]\n",
      "389 [D loss: 0.00, acc.: 100.00%] [Q loss: 0.79] [G loss: 0.05]\n",
      "390 [D loss: 0.00, acc.: 100.00%] [Q loss: 0.82] [G loss: 0.10]\n",
      "391 [D loss: 0.00, acc.: 100.00%] [Q loss: 0.68] [G loss: 0.07]\n",
      "392 [D loss: 0.01, acc.: 100.00%] [Q loss: 0.77] [G loss: 0.12]\n",
      "393 [D loss: 0.01, acc.: 100.00%] [Q loss: 0.80] [G loss: 0.04]\n",
      "394 [D loss: 0.00, acc.: 100.00%] [Q loss: 0.83] [G loss: 0.05]\n",
      "395 [D loss: 0.01, acc.: 100.00%] [Q loss: 0.78] [G loss: 0.05]\n",
      "396 [D loss: 0.01, acc.: 100.00%] [Q loss: 0.85] [G loss: 0.08]\n",
      "397 [D loss: 0.00, acc.: 100.00%] [Q loss: 0.90] [G loss: 0.07]\n",
      "398 [D loss: 0.00, acc.: 100.00%] [Q loss: 0.90] [G loss: 0.11]\n",
      "399 [D loss: 0.01, acc.: 100.00%] [Q loss: 0.79] [G loss: 0.05]\n",
      "400 [D loss: 0.01, acc.: 100.00%] [Q loss: 0.79] [G loss: 0.07]\n",
      "401 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.01] [G loss: 0.07]\n",
      "402 [D loss: 0.02, acc.: 99.61%] [Q loss: 0.48] [G loss: 0.07]\n",
      "403 [D loss: 0.00, acc.: 100.00%] [Q loss: 0.64] [G loss: 0.07]\n",
      "404 [D loss: 0.00, acc.: 100.00%] [Q loss: 0.69] [G loss: 0.16]\n",
      "405 [D loss: 0.01, acc.: 100.00%] [Q loss: 1.09] [G loss: 0.09]\n",
      "406 [D loss: 0.01, acc.: 100.00%] [Q loss: 1.11] [G loss: 0.09]\n",
      "407 [D loss: 0.01, acc.: 100.00%] [Q loss: 0.88] [G loss: 0.08]\n",
      "408 [D loss: 0.00, acc.: 100.00%] [Q loss: 0.71] [G loss: 0.08]\n",
      "409 [D loss: 0.00, acc.: 100.00%] [Q loss: 0.70] [G loss: 0.05]\n",
      "410 [D loss: 0.01, acc.: 100.00%] [Q loss: 0.81] [G loss: 0.09]\n",
      "411 [D loss: 0.00, acc.: 100.00%] [Q loss: 0.96] [G loss: 0.11]\n",
      "412 [D loss: 0.00, acc.: 100.00%] [Q loss: 0.69] [G loss: 0.08]\n",
      "413 [D loss: 0.00, acc.: 100.00%] [Q loss: 0.73] [G loss: 0.12]\n",
      "414 [D loss: 0.01, acc.: 100.00%] [Q loss: 0.78] [G loss: 0.07]\n",
      "415 [D loss: 0.00, acc.: 100.00%] [Q loss: 0.81] [G loss: 0.13]\n",
      "416 [D loss: 0.00, acc.: 100.00%] [Q loss: 0.81] [G loss: 0.10]\n",
      "417 [D loss: 0.00, acc.: 100.00%] [Q loss: 0.77] [G loss: 0.05]\n",
      "418 [D loss: 0.00, acc.: 100.00%] [Q loss: 0.68] [G loss: 0.06]\n",
      "419 [D loss: 0.01, acc.: 100.00%] [Q loss: 0.63] [G loss: 0.17]\n",
      "420 [D loss: 0.01, acc.: 100.00%] [Q loss: 0.70] [G loss: 0.05]\n",
      "421 [D loss: 0.01, acc.: 100.00%] [Q loss: 1.02] [G loss: 0.13]\n",
      "422 [D loss: 0.01, acc.: 100.00%] [Q loss: 0.95] [G loss: 0.13]\n",
      "423 [D loss: 0.01, acc.: 100.00%] [Q loss: 0.91] [G loss: 0.14]\n",
      "424 [D loss: 0.01, acc.: 100.00%] [Q loss: 0.92] [G loss: 0.16]\n",
      "425 [D loss: 0.00, acc.: 100.00%] [Q loss: 0.78] [G loss: 0.09]\n",
      "426 [D loss: 0.01, acc.: 100.00%] [Q loss: 0.63] [G loss: 0.11]\n",
      "427 [D loss: 0.01, acc.: 100.00%] [Q loss: 0.76] [G loss: 0.12]\n",
      "428 [D loss: 0.00, acc.: 100.00%] [Q loss: 0.89] [G loss: 0.04]\n",
      "429 [D loss: 0.00, acc.: 100.00%] [Q loss: 0.73] [G loss: 0.10]\n",
      "430 [D loss: 0.00, acc.: 100.00%] [Q loss: 0.76] [G loss: 0.11]\n",
      "431 [D loss: 0.00, acc.: 100.00%] [Q loss: 0.79] [G loss: 0.12]\n",
      "432 [D loss: 0.00, acc.: 100.00%] [Q loss: 0.87] [G loss: 0.08]\n",
      "433 [D loss: 0.01, acc.: 100.00%] [Q loss: 0.75] [G loss: 0.04]\n",
      "434 [D loss: 0.00, acc.: 100.00%] [Q loss: 0.74] [G loss: 0.06]\n",
      "435 [D loss: 0.00, acc.: 100.00%] [Q loss: 0.78] [G loss: 0.08]\n",
      "436 [D loss: 0.00, acc.: 100.00%] [Q loss: 0.85] [G loss: 0.05]\n",
      "437 [D loss: 0.01, acc.: 100.00%] [Q loss: 0.65] [G loss: 0.12]\n",
      "438 [D loss: 0.00, acc.: 100.00%] [Q loss: 0.82] [G loss: 0.07]\n",
      "439 [D loss: 0.00, acc.: 100.00%] [Q loss: 0.87] [G loss: 0.10]\n",
      "440 [D loss: 0.00, acc.: 100.00%] [Q loss: 0.62] [G loss: 0.10]\n",
      "441 [D loss: 0.00, acc.: 100.00%] [Q loss: 0.83] [G loss: 0.10]\n",
      "442 [D loss: 0.00, acc.: 100.00%] [Q loss: 0.85] [G loss: 0.08]\n",
      "443 [D loss: 0.00, acc.: 100.00%] [Q loss: 0.84] [G loss: 0.04]\n",
      "444 [D loss: 0.02, acc.: 99.61%] [Q loss: 1.51] [G loss: 0.08]\n",
      "445 [D loss: 0.01, acc.: 100.00%] [Q loss: 1.05] [G loss: 0.11]\n",
      "446 [D loss: 0.00, acc.: 100.00%] [Q loss: 0.75] [G loss: 0.09]\n",
      "447 [D loss: 0.00, acc.: 100.00%] [Q loss: 0.78] [G loss: 0.05]\n",
      "448 [D loss: 0.00, acc.: 100.00%] [Q loss: 0.86] [G loss: 0.14]\n",
      "449 [D loss: 0.00, acc.: 100.00%] [Q loss: 0.93] [G loss: 0.09]\n",
      "450 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.03] [G loss: 0.13]\n",
      "451 [D loss: 0.00, acc.: 100.00%] [Q loss: 0.84] [G loss: 0.11]\n",
      "452 [D loss: 0.00, acc.: 100.00%] [Q loss: 0.93] [G loss: 0.05]\n",
      "453 [D loss: 0.00, acc.: 100.00%] [Q loss: 0.91] [G loss: 0.10]\n",
      "454 [D loss: 0.00, acc.: 100.00%] [Q loss: 0.80] [G loss: 0.10]\n",
      "455 [D loss: 0.00, acc.: 100.00%] [Q loss: 0.93] [G loss: 0.05]\n",
      "456 [D loss: 0.01, acc.: 100.00%] [Q loss: 0.84] [G loss: 0.09]\n",
      "457 [D loss: 0.00, acc.: 100.00%] [Q loss: 0.73] [G loss: 0.09]\n",
      "458 [D loss: 0.00, acc.: 100.00%] [Q loss: 0.90] [G loss: 0.10]\n",
      "459 [D loss: 0.00, acc.: 100.00%] [Q loss: 0.77] [G loss: 0.13]\n",
      "460 [D loss: 0.00, acc.: 100.00%] [Q loss: 0.85] [G loss: 0.15]\n",
      "461 [D loss: 0.00, acc.: 100.00%] [Q loss: 0.88] [G loss: 0.05]\n",
      "462 [D loss: 0.00, acc.: 100.00%] [Q loss: 0.73] [G loss: 0.07]\n",
      "463 [D loss: 0.00, acc.: 100.00%] [Q loss: 0.95] [G loss: 0.09]\n",
      "464 [D loss: 0.00, acc.: 100.00%] [Q loss: 0.81] [G loss: 0.12]\n",
      "465 [D loss: 0.00, acc.: 100.00%] [Q loss: 0.85] [G loss: 0.14]\n",
      "466 [D loss: 0.00, acc.: 100.00%] [Q loss: 0.92] [G loss: 0.04]\n",
      "467 [D loss: 0.00, acc.: 100.00%] [Q loss: 0.92] [G loss: 0.12]\n",
      "468 [D loss: 0.00, acc.: 100.00%] [Q loss: 0.95] [G loss: 0.11]\n",
      "469 [D loss: 0.00, acc.: 100.00%] [Q loss: 0.74] [G loss: 0.08]\n",
      "470 [D loss: 0.01, acc.: 100.00%] [Q loss: 0.97] [G loss: 0.06]\n",
      "471 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.17] [G loss: 0.06]\n",
      "472 [D loss: 0.01, acc.: 100.00%] [Q loss: 0.90] [G loss: 0.06]\n",
      "473 [D loss: 0.00, acc.: 100.00%] [Q loss: 0.85] [G loss: 0.04]\n",
      "474 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.10] [G loss: 0.06]\n",
      "475 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.30] [G loss: 0.08]\n",
      "476 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.14] [G loss: 0.06]\n",
      "477 [D loss: 0.00, acc.: 100.00%] [Q loss: 0.96] [G loss: 0.08]\n",
      "478 [D loss: 0.00, acc.: 100.00%] [Q loss: 0.93] [G loss: 0.06]\n",
      "479 [D loss: 0.00, acc.: 100.00%] [Q loss: 0.89] [G loss: 0.06]\n",
      "480 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.01] [G loss: 0.06]\n",
      "481 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.15] [G loss: 0.05]\n",
      "482 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.08] [G loss: 0.05]\n",
      "483 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.08] [G loss: 0.05]\n",
      "484 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.02] [G loss: 0.06]\n",
      "485 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.11] [G loss: 0.06]\n",
      "486 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.03] [G loss: 0.05]\n",
      "487 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.19] [G loss: 0.13]\n",
      "488 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.14] [G loss: 0.10]\n",
      "489 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.02] [G loss: 0.07]\n",
      "490 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.14] [G loss: 0.11]\n",
      "491 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.09] [G loss: 0.10]\n",
      "492 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.08] [G loss: 0.09]\n",
      "493 [D loss: 0.00, acc.: 100.00%] [Q loss: 0.93] [G loss: 0.09]\n",
      "494 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.03] [G loss: 0.06]\n",
      "495 [D loss: 0.01, acc.: 100.00%] [Q loss: 0.83] [G loss: 0.10]\n",
      "496 [D loss: 0.01, acc.: 100.00%] [Q loss: 1.08] [G loss: 0.05]\n",
      "497 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.40] [G loss: 0.06]\n",
      "498 [D loss: 0.01, acc.: 100.00%] [Q loss: 0.61] [G loss: 0.09]\n",
      "499 [D loss: 0.00, acc.: 100.00%] [Q loss: 0.76] [G loss: 0.04]\n",
      "500 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.03] [G loss: 0.13]\n",
      "501 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.01] [G loss: 0.06]\n",
      "502 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.11] [G loss: 0.06]\n",
      "503 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.05] [G loss: 0.04]\n",
      "504 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.18] [G loss: 0.24]\n",
      "505 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.00] [G loss: 0.05]\n",
      "506 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.03] [G loss: 0.06]\n",
      "507 [D loss: 0.00, acc.: 100.00%] [Q loss: 0.95] [G loss: 0.11]\n",
      "508 [D loss: 0.00, acc.: 100.00%] [Q loss: 0.89] [G loss: 0.08]\n",
      "509 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.03] [G loss: 0.10]\n",
      "510 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.11] [G loss: 0.05]\n",
      "511 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.24] [G loss: 0.04]\n",
      "512 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.03] [G loss: 0.05]\n",
      "513 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.04] [G loss: 0.15]\n",
      "514 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.07] [G loss: 0.08]\n",
      "515 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.32] [G loss: 0.14]\n",
      "516 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.06] [G loss: 0.06]\n",
      "517 [D loss: 0.00, acc.: 100.00%] [Q loss: 0.95] [G loss: 0.06]\n",
      "518 [D loss: 0.00, acc.: 100.00%] [Q loss: 0.92] [G loss: 0.13]\n",
      "519 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.24] [G loss: 0.06]\n",
      "520 [D loss: 0.01, acc.: 100.00%] [Q loss: 0.78] [G loss: 0.10]\n",
      "521 [D loss: 0.00, acc.: 100.00%] [Q loss: 0.91] [G loss: 0.06]\n",
      "522 [D loss: 0.00, acc.: 100.00%] [Q loss: 0.96] [G loss: 0.05]\n",
      "523 [D loss: 0.01, acc.: 100.00%] [Q loss: 0.87] [G loss: 0.06]\n",
      "524 [D loss: 0.00, acc.: 100.00%] [Q loss: 0.97] [G loss: 0.07]\n",
      "525 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.11] [G loss: 0.04]\n",
      "526 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.13] [G loss: 0.08]\n",
      "527 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.08] [G loss: 0.03]\n",
      "528 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.03] [G loss: 0.08]\n",
      "529 [D loss: 0.00, acc.: 100.00%] [Q loss: 0.99] [G loss: 0.08]\n",
      "530 [D loss: 0.00, acc.: 100.00%] [Q loss: 0.96] [G loss: 0.07]\n",
      "531 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.10] [G loss: 0.05]\n",
      "532 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.28] [G loss: 0.08]\n",
      "533 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.01] [G loss: 0.09]\n",
      "534 [D loss: 0.00, acc.: 100.00%] [Q loss: 0.93] [G loss: 0.04]\n",
      "535 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.08] [G loss: 0.06]\n",
      "536 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.06] [G loss: 0.04]\n",
      "537 [D loss: 0.00, acc.: 100.00%] [Q loss: 0.80] [G loss: 0.06]\n",
      "538 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.01] [G loss: 0.05]\n",
      "539 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.03] [G loss: 0.05]\n",
      "540 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.16] [G loss: 0.08]\n",
      "541 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.02] [G loss: 0.08]\n",
      "542 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.06] [G loss: 0.02]\n",
      "543 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.17] [G loss: 0.09]\n",
      "544 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.16] [G loss: 0.09]\n",
      "545 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.10] [G loss: 0.12]\n",
      "546 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.29] [G loss: 0.08]\n",
      "547 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.17] [G loss: 0.09]\n",
      "548 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.20] [G loss: 0.04]\n",
      "549 [D loss: 0.00, acc.: 100.00%] [Q loss: 0.85] [G loss: 0.06]\n",
      "550 [D loss: 0.00, acc.: 100.00%] [Q loss: 0.95] [G loss: 0.11]\n",
      "551 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.10] [G loss: 0.09]\n",
      "552 [D loss: 0.00, acc.: 100.00%] [Q loss: 0.92] [G loss: 0.06]\n",
      "553 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.15] [G loss: 0.09]\n",
      "554 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.02] [G loss: 0.05]\n",
      "555 [D loss: 0.00, acc.: 100.00%] [Q loss: 0.99] [G loss: 0.11]\n",
      "556 [D loss: 0.00, acc.: 100.00%] [Q loss: 0.98] [G loss: 0.03]\n",
      "557 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.14] [G loss: 0.07]\n",
      "558 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.30] [G loss: 0.05]\n",
      "559 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.12] [G loss: 0.05]\n",
      "560 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.04] [G loss: 0.07]\n",
      "561 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.05] [G loss: 0.08]\n",
      "562 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.20] [G loss: 0.06]\n",
      "563 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.16] [G loss: 0.05]\n",
      "564 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.23] [G loss: 0.09]\n",
      "565 [D loss: 0.01, acc.: 100.00%] [Q loss: 0.95] [G loss: 0.08]\n",
      "566 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.52] [G loss: 0.05]\n",
      "567 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.08] [G loss: 0.04]\n",
      "568 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.22] [G loss: 0.07]\n",
      "569 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.14] [G loss: 0.06]\n",
      "570 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.29] [G loss: 0.10]\n",
      "571 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.12] [G loss: 0.08]\n",
      "572 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.27] [G loss: 0.02]\n",
      "573 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.21] [G loss: 0.07]\n",
      "574 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.38] [G loss: 0.09]\n",
      "575 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.24] [G loss: 0.03]\n",
      "576 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.13] [G loss: 0.04]\n",
      "577 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.15] [G loss: 0.10]\n",
      "578 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.14] [G loss: 0.07]\n",
      "579 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.15] [G loss: 0.08]\n",
      "580 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.45] [G loss: 0.05]\n",
      "581 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.14] [G loss: 0.06]\n",
      "582 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.12] [G loss: 0.07]\n",
      "583 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.24] [G loss: 0.06]\n",
      "584 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.21] [G loss: 0.08]\n",
      "585 [D loss: 0.01, acc.: 100.00%] [Q loss: 1.33] [G loss: 0.06]\n",
      "586 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.30] [G loss: 0.07]\n",
      "587 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.25] [G loss: 0.06]\n",
      "588 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.23] [G loss: 0.07]\n",
      "589 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.10] [G loss: 0.05]\n",
      "590 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.07] [G loss: 0.10]\n",
      "591 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.28] [G loss: 0.12]\n",
      "592 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.18] [G loss: 0.06]\n",
      "593 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.22] [G loss: 0.09]\n",
      "594 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.32] [G loss: 0.04]\n",
      "595 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.15] [G loss: 0.03]\n",
      "596 [D loss: 0.00, acc.: 100.00%] [Q loss: 0.95] [G loss: 0.10]\n",
      "597 [D loss: 0.00, acc.: 100.00%] [Q loss: 0.95] [G loss: 0.05]\n",
      "598 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.16] [G loss: 0.07]\n",
      "599 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.14] [G loss: 0.06]\n",
      "600 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.24] [G loss: 0.08]\n",
      "601 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.55] [G loss: 0.09]\n",
      "602 [D loss: 0.01, acc.: 100.00%] [Q loss: 0.69] [G loss: 0.03]\n",
      "603 [D loss: 0.00, acc.: 100.00%] [Q loss: 0.83] [G loss: 0.09]\n",
      "604 [D loss: 0.00, acc.: 100.00%] [Q loss: 0.85] [G loss: 0.04]\n",
      "605 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.07] [G loss: 0.06]\n",
      "606 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.15] [G loss: 0.08]\n",
      "607 [D loss: 0.00, acc.: 100.00%] [Q loss: 0.85] [G loss: 0.07]\n",
      "608 [D loss: 0.00, acc.: 100.00%] [Q loss: 0.84] [G loss: 0.03]\n",
      "609 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.08] [G loss: 0.03]\n",
      "610 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.46] [G loss: 0.07]\n",
      "611 [D loss: 0.01, acc.: 100.00%] [Q loss: 1.06] [G loss: 0.08]\n",
      "612 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.10] [G loss: 0.09]\n",
      "613 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.04] [G loss: 0.07]\n",
      "614 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.03] [G loss: 0.06]\n",
      "615 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.11] [G loss: 0.05]\n",
      "616 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.17] [G loss: 0.07]\n",
      "617 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.29] [G loss: 0.08]\n",
      "618 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.29] [G loss: 0.04]\n",
      "619 [D loss: 0.01, acc.: 100.00%] [Q loss: 0.92] [G loss: 0.07]\n",
      "620 [D loss: 0.00, acc.: 100.00%] [Q loss: 0.85] [G loss: 0.08]\n",
      "621 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.20] [G loss: 0.07]\n",
      "622 [D loss: 0.00, acc.: 100.00%] [Q loss: 0.96] [G loss: 0.06]\n",
      "623 [D loss: 0.00, acc.: 100.00%] [Q loss: 0.86] [G loss: 0.04]\n",
      "624 [D loss: 0.00, acc.: 100.00%] [Q loss: 0.96] [G loss: 0.09]\n",
      "625 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.00] [G loss: 0.07]\n",
      "626 [D loss: 0.00, acc.: 100.00%] [Q loss: 0.79] [G loss: 0.09]\n",
      "627 [D loss: 0.00, acc.: 100.00%] [Q loss: 0.92] [G loss: 0.07]\n",
      "628 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.26] [G loss: 0.05]\n",
      "629 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.31] [G loss: 0.04]\n",
      "630 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.19] [G loss: 0.01]\n",
      "631 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.10] [G loss: 0.06]\n",
      "632 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.15] [G loss: 0.08]\n",
      "633 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.05] [G loss: 0.04]\n",
      "634 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.08] [G loss: 0.07]\n",
      "635 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.21] [G loss: 0.05]\n",
      "636 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.30] [G loss: 0.04]\n",
      "637 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.21] [G loss: 0.07]\n",
      "638 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.10] [G loss: 0.05]\n",
      "639 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.07] [G loss: 0.04]\n",
      "640 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.18] [G loss: 0.06]\n",
      "641 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.20] [G loss: 0.06]\n",
      "642 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.12] [G loss: 0.06]\n",
      "643 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.16] [G loss: 0.06]\n",
      "644 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.27] [G loss: 0.04]\n",
      "645 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.16] [G loss: 0.02]\n",
      "646 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.04] [G loss: 0.05]\n",
      "647 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.06] [G loss: 0.06]\n",
      "648 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.20] [G loss: 0.07]\n",
      "649 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.31] [G loss: 0.06]\n",
      "650 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.15] [G loss: 0.02]\n",
      "651 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.05] [G loss: 0.09]\n",
      "652 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.06] [G loss: 0.02]\n",
      "653 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.17] [G loss: 0.04]\n",
      "654 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.12] [G loss: 0.07]\n",
      "655 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.10] [G loss: 0.11]\n",
      "656 [D loss: 0.00, acc.: 100.00%] [Q loss: 0.95] [G loss: 0.13]\n",
      "657 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.08] [G loss: 0.06]\n",
      "658 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.09] [G loss: 0.10]\n",
      "659 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.00] [G loss: 0.04]\n",
      "660 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.26] [G loss: 0.05]\n",
      "661 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.45] [G loss: 0.03]\n",
      "662 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.61] [G loss: 0.04]\n",
      "663 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.20] [G loss: 0.04]\n",
      "664 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.30] [G loss: 0.03]\n",
      "665 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.33] [G loss: 0.04]\n",
      "666 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.50] [G loss: 0.03]\n",
      "667 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.37] [G loss: 0.05]\n",
      "668 [D loss: 0.00, acc.: 100.00%] [Q loss: 1.50] [G loss: 0.03]\n",
      "669 [D loss: 0.03, acc.: 99.61%] [Q loss: 8.45] [G loss: 0.69]\n",
      "670 [D loss: 15.86, acc.: 0.00%] [Q loss: 0.00] [G loss: 4.16]\n",
      "671 [D loss: 7.55, acc.: 50.00%] [Q loss: 0.13] [G loss: 3.92]\n",
      "672 [D loss: 3.44, acc.: 50.00%] [Q loss: 3.39] [G loss: 2.23]\n",
      "673 [D loss: 0.85, acc.: 79.69%] [Q loss: 3.32] [G loss: 2.48]\n",
      "674 [D loss: 1.07, acc.: 53.91%] [Q loss: 0.42] [G loss: 2.02]\n",
      "675 [D loss: 0.48, acc.: 82.03%] [Q loss: 0.32] [G loss: 1.58]\n",
      "676 [D loss: 0.10, acc.: 98.83%] [Q loss: 0.47] [G loss: 1.48]\n",
      "677 [D loss: 0.06, acc.: 100.00%] [Q loss: 0.74] [G loss: 0.93]\n",
      "678 [D loss: 0.06, acc.: 99.22%] [Q loss: 0.83] [G loss: 0.51]\n",
      "679 [D loss: 0.06, acc.: 99.22%] [Q loss: 0.83] [G loss: 0.33]\n",
      "680 [D loss: 0.04, acc.: 100.00%] [Q loss: 0.87] [G loss: 0.15]\n",
      "681 [D loss: 0.04, acc.: 100.00%] [Q loss: 0.81] [G loss: 0.24]\n",
      "682 [D loss: 0.05, acc.: 98.83%] [Q loss: 0.85] [G loss: 0.12]\n",
      "683 [D loss: 0.03, acc.: 100.00%] [Q loss: 0.92] [G loss: 0.22]\n",
      "684 [D loss: 0.04, acc.: 99.61%] [Q loss: 0.99] [G loss: 0.10]\n",
      "685 [D loss: 0.06, acc.: 99.61%] [Q loss: 0.81] [G loss: 0.12]\n",
      "686 [D loss: 0.05, acc.: 100.00%] [Q loss: 0.93] [G loss: 0.15]\n",
      "687 [D loss: 0.05, acc.: 100.00%] [Q loss: 0.92] [G loss: 0.12]\n",
      "688 [D loss: 0.04, acc.: 100.00%] [Q loss: 1.04] [G loss: 0.18]\n",
      "689 [D loss: 0.07, acc.: 99.22%] [Q loss: 0.89] [G loss: 0.11]\n",
      "690 [D loss: 0.04, acc.: 100.00%] [Q loss: 1.05] [G loss: 0.07]\n",
      "691 [D loss: 0.05, acc.: 99.61%] [Q loss: 0.93] [G loss: 0.10]\n",
      "692 [D loss: 0.03, acc.: 100.00%] [Q loss: 1.00] [G loss: 0.13]\n",
      "693 [D loss: 0.04, acc.: 100.00%] [Q loss: 0.99] [G loss: 0.09]\n",
      "694 [D loss: 0.03, acc.: 100.00%] [Q loss: 1.06] [G loss: 0.07]\n",
      "695 [D loss: 0.05, acc.: 99.61%] [Q loss: 0.98] [G loss: 0.05]\n",
      "696 [D loss: 0.03, acc.: 100.00%] [Q loss: 0.97] [G loss: 0.10]\n",
      "697 [D loss: 0.04, acc.: 100.00%] [Q loss: 0.90] [G loss: 0.04]\n",
      "698 [D loss: 0.06, acc.: 100.00%] [Q loss: 0.91] [G loss: 0.06]\n",
      "699 [D loss: 0.06, acc.: 100.00%] [Q loss: 0.83] [G loss: 0.10]\n",
      "700 [D loss: 0.05, acc.: 100.00%] [Q loss: 0.80] [G loss: 0.04]\n",
      "701 [D loss: 0.11, acc.: 99.22%] [Q loss: 0.78] [G loss: 0.06]\n",
      "702 [D loss: 0.11, acc.: 98.83%] [Q loss: 0.89] [G loss: 0.06]\n",
      "703 [D loss: 0.10, acc.: 100.00%] [Q loss: 0.90] [G loss: 0.05]\n",
      "704 [D loss: 0.12, acc.: 98.83%] [Q loss: 0.75] [G loss: 0.06]\n",
      "705 [D loss: 0.20, acc.: 95.31%] [Q loss: 0.77] [G loss: 0.11]\n",
      "706 [D loss: 0.17, acc.: 97.66%] [Q loss: 0.69] [G loss: 0.05]\n",
      "707 [D loss: 0.14, acc.: 99.22%] [Q loss: 0.56] [G loss: 0.04]\n",
      "708 [D loss: 0.11, acc.: 99.22%] [Q loss: 0.77] [G loss: 0.06]\n",
      "709 [D loss: 0.21, acc.: 96.48%] [Q loss: 0.63] [G loss: 0.07]\n",
      "710 [D loss: 0.28, acc.: 89.06%] [Q loss: 0.97] [G loss: 0.05]\n",
      "711 [D loss: 0.18, acc.: 97.27%] [Q loss: 0.78] [G loss: 0.08]\n",
      "712 [D loss: 0.16, acc.: 99.22%] [Q loss: 0.64] [G loss: 0.07]\n",
      "713 [D loss: 0.19, acc.: 96.09%] [Q loss: 0.63] [G loss: 0.07]\n",
      "714 [D loss: 0.14, acc.: 98.83%] [Q loss: 0.64] [G loss: 0.11]\n",
      "715 [D loss: 0.17, acc.: 98.05%] [Q loss: 0.48] [G loss: 0.04]\n",
      "716 [D loss: 0.11, acc.: 99.61%] [Q loss: 0.54] [G loss: 0.05]\n",
      "717 [D loss: 0.09, acc.: 100.00%] [Q loss: 0.51] [G loss: 0.04]\n",
      "718 [D loss: 0.07, acc.: 100.00%] [Q loss: 0.53] [G loss: 0.03]\n",
      "719 [D loss: 0.11, acc.: 99.22%] [Q loss: 0.50] [G loss: 0.03]\n",
      "720 [D loss: 0.10, acc.: 99.61%] [Q loss: 0.56] [G loss: 0.08]\n",
      "721 [D loss: 0.09, acc.: 100.00%] [Q loss: 0.56] [G loss: 0.07]\n",
      "722 [D loss: 0.07, acc.: 100.00%] [Q loss: 0.51] [G loss: 0.08]\n",
      "723 [D loss: 0.06, acc.: 99.61%] [Q loss: 0.52] [G loss: 0.04]\n",
      "724 [D loss: 0.07, acc.: 100.00%] [Q loss: 0.45] [G loss: 0.03]\n",
      "725 [D loss: 0.08, acc.: 99.61%] [Q loss: 0.48] [G loss: 0.04]\n",
      "726 [D loss: 0.08, acc.: 99.22%] [Q loss: 0.50] [G loss: 0.05]\n",
      "727 [D loss: 0.06, acc.: 100.00%] [Q loss: 0.47] [G loss: 0.10]\n",
      "728 [D loss: 0.06, acc.: 100.00%] [Q loss: 0.43] [G loss: 0.05]\n",
      "729 [D loss: 0.04, acc.: 99.61%] [Q loss: 0.39] [G loss: 0.05]\n",
      "730 [D loss: 0.05, acc.: 100.00%] [Q loss: 0.41] [G loss: 0.05]\n",
      "731 [D loss: 0.05, acc.: 100.00%] [Q loss: 0.53] [G loss: 0.02]\n",
      "732 [D loss: 0.05, acc.: 100.00%] [Q loss: 0.48] [G loss: 0.04]\n",
      "733 [D loss: 0.09, acc.: 99.61%] [Q loss: 0.38] [G loss: 0.06]\n",
      "734 [D loss: 0.03, acc.: 100.00%] [Q loss: 0.42] [G loss: 0.07]\n",
      "735 [D loss: 0.03, acc.: 100.00%] [Q loss: 0.45] [G loss: 0.04]\n",
      "736 [D loss: 0.04, acc.: 100.00%] [Q loss: 0.40] [G loss: 0.05]\n",
      "737 [D loss: 0.04, acc.: 100.00%] [Q loss: 0.35] [G loss: 0.04]\n",
      "738 [D loss: 0.03, acc.: 100.00%] [Q loss: 0.42] [G loss: 0.07]\n",
      "739 [D loss: 0.04, acc.: 100.00%] [Q loss: 0.38] [G loss: 0.04]\n",
      "740 [D loss: 0.03, acc.: 100.00%] [Q loss: 0.41] [G loss: 0.03]\n",
      "741 [D loss: 0.03, acc.: 100.00%] [Q loss: 0.37] [G loss: 0.03]\n",
      "742 [D loss: 0.02, acc.: 100.00%] [Q loss: 0.38] [G loss: 0.08]\n",
      "743 [D loss: 0.04, acc.: 100.00%] [Q loss: 0.35] [G loss: 0.03]\n",
      "744 [D loss: 0.05, acc.: 100.00%] [Q loss: 0.42] [G loss: 0.04]\n",
      "745 [D loss: 0.01, acc.: 100.00%] [Q loss: 0.54] [G loss: 0.07]\n",
      "746 [D loss: 0.02, acc.: 99.61%] [Q loss: 0.42] [G loss: 0.14]\n",
      "747 [D loss: 0.03, acc.: 100.00%] [Q loss: 0.55] [G loss: 0.05]\n",
      "748 [D loss: 0.03, acc.: 100.00%] [Q loss: 0.65] [G loss: 0.04]\n",
      "749 [D loss: 0.02, acc.: 100.00%] [Q loss: 0.64] [G loss: 0.03]\n",
      "750 [D loss: 0.02, acc.: 100.00%] [Q loss: 0.43] [G loss: 0.03]\n",
      "751 [D loss: 0.01, acc.: 100.00%] [Q loss: 0.36] [G loss: 0.28]\n",
      "752 [D loss: 0.02, acc.: 100.00%] [Q loss: 0.81] [G loss: 0.08]\n",
      "753 [D loss: 0.03, acc.: 100.00%] [Q loss: 0.83] [G loss: 0.02]\n",
      "754 [D loss: 0.02, acc.: 100.00%] [Q loss: 0.78] [G loss: 0.03]\n",
      "755 [D loss: 0.02, acc.: 100.00%] [Q loss: 0.52] [G loss: 0.04]\n",
      "756 [D loss: 0.01, acc.: 100.00%] [Q loss: 0.37] [G loss: 0.20]\n",
      "757 [D loss: 0.01, acc.: 100.00%] [Q loss: 0.51] [G loss: 0.02]\n",
      "758 [D loss: 0.02, acc.: 100.00%] [Q loss: 0.44] [G loss: 0.03]\n",
      "759 [D loss: 0.01, acc.: 100.00%] [Q loss: 0.39] [G loss: 0.05]\n",
      "760 [D loss: 0.01, acc.: 100.00%] [Q loss: 0.39] [G loss: 0.12]\n",
      "761 [D loss: 0.02, acc.: 100.00%] [Q loss: 0.49] [G loss: 0.02]\n",
      "762 [D loss: 0.02, acc.: 100.00%] [Q loss: 0.46] [G loss: 0.03]\n",
      "763 [D loss: 0.02, acc.: 100.00%] [Q loss: 0.40] [G loss: 0.09]\n",
      "764 [D loss: 0.01, acc.: 100.00%] [Q loss: 0.50] [G loss: 0.03]\n",
      "765 [D loss: 0.02, acc.: 100.00%] [Q loss: 0.39] [G loss: 0.10]\n",
      "766 [D loss: 0.02, acc.: 100.00%] [Q loss: 0.45] [G loss: 0.05]\n",
      "767 [D loss: 0.02, acc.: 100.00%] [Q loss: 0.46] [G loss: 0.03]\n",
      "768 [D loss: 0.02, acc.: 99.61%] [Q loss: 0.53] [G loss: 0.04]\n",
      "769 [D loss: 0.01, acc.: 100.00%] [Q loss: 0.54] [G loss: 0.09]\n",
      "770 [D loss: 0.02, acc.: 100.00%] [Q loss: 0.50] [G loss: 0.05]\n",
      "771 [D loss: 0.01, acc.: 100.00%] [Q loss: 0.50] [G loss: 0.03]\n",
      "772 [D loss: 0.02, acc.: 100.00%] [Q loss: 0.48] [G loss: 0.04]\n",
      "773 [D loss: 0.01, acc.: 100.00%] [Q loss: 0.48] [G loss: 0.07]\n",
      "774 [D loss: 0.01, acc.: 100.00%] [Q loss: 0.51] [G loss: 0.04]\n",
      "775 [D loss: 0.02, acc.: 100.00%] [Q loss: 0.61] [G loss: 0.02]\n",
      "776 [D loss: 0.02, acc.: 100.00%] [Q loss: 0.52] [G loss: 0.04]\n",
      "777 [D loss: 0.02, acc.: 100.00%] [Q loss: 0.53] [G loss: 0.04]\n",
      "778 [D loss: 0.02, acc.: 100.00%] [Q loss: 0.58] [G loss: 0.03]\n",
      "779 [D loss: 0.04, acc.: 99.61%] [Q loss: 0.56] [G loss: 0.05]\n",
      "780 [D loss: 0.02, acc.: 100.00%] [Q loss: 0.71] [G loss: 0.07]\n",
      "781 [D loss: 0.01, acc.: 100.00%] [Q loss: 0.68] [G loss: 0.03]\n",
      "782 [D loss: 0.02, acc.: 100.00%] [Q loss: 0.56] [G loss: 0.08]\n",
      "783 [D loss: 0.02, acc.: 100.00%] [Q loss: 0.54] [G loss: 0.02]\n",
      "784 [D loss: 0.01, acc.: 100.00%] [Q loss: 0.63] [G loss: 0.03]\n",
      "785 [D loss: 0.01, acc.: 100.00%] [Q loss: 0.62] [G loss: 0.04]\n",
      "786 [D loss: 0.02, acc.: 100.00%] [Q loss: 0.54] [G loss: 0.07]\n",
      "787 [D loss: 0.02, acc.: 100.00%] [Q loss: 0.67] [G loss: 0.02]\n",
      "788 [D loss: 0.02, acc.: 100.00%] [Q loss: 0.66] [G loss: 0.03]\n",
      "789 [D loss: 0.02, acc.: 100.00%] [Q loss: 0.67] [G loss: 0.03]\n",
      "790 [D loss: 0.02, acc.: 100.00%] [Q loss: 0.54] [G loss: 0.09]\n",
      "791 [D loss: 0.03, acc.: 100.00%] [Q loss: 0.66] [G loss: 0.05]\n",
      "792 [D loss: 0.02, acc.: 100.00%] [Q loss: 0.75] [G loss: 0.03]\n",
      "793 [D loss: 0.01, acc.: 100.00%] [Q loss: 0.69] [G loss: 0.03]\n",
      "794 [D loss: 0.02, acc.: 100.00%] [Q loss: 0.62] [G loss: 0.06]\n",
      "795 [D loss: 0.02, acc.: 100.00%] [Q loss: 0.71] [G loss: 0.05]\n",
      "796 [D loss: 0.01, acc.: 100.00%] [Q loss: 0.69] [G loss: 0.04]\n",
      "797 [D loss: 0.01, acc.: 100.00%] [Q loss: 0.65] [G loss: 0.05]\n",
      "798 [D loss: 0.01, acc.: 100.00%] [Q loss: 0.75] [G loss: 0.03]\n",
      "799 [D loss: 0.02, acc.: 100.00%] [Q loss: 0.77] [G loss: 0.03]\n",
      "800 [D loss: 0.02, acc.: 100.00%] [Q loss: 0.99] [G loss: 0.03]\n",
      "801 [D loss: 0.01, acc.: 100.00%] [Q loss: 0.93] [G loss: 0.06]\n",
      "802 [D loss: 0.02, acc.: 100.00%] [Q loss: 0.70] [G loss: 0.02]\n",
      "803 [D loss: 0.02, acc.: 100.00%] [Q loss: 0.95] [G loss: 0.06]\n",
      "804 [D loss: 0.01, acc.: 100.00%] [Q loss: 1.07] [G loss: 0.07]\n",
      "805 [D loss: 0.02, acc.: 100.00%] [Q loss: 0.99] [G loss: 0.05]\n",
      "806 [D loss: 0.01, acc.: 100.00%] [Q loss: 1.15] [G loss: 0.04]\n",
      "807 [D loss: 0.03, acc.: 100.00%] [Q loss: 0.97] [G loss: 0.05]\n",
      "808 [D loss: 0.02, acc.: 100.00%] [Q loss: 1.05] [G loss: 0.03]\n",
      "809 [D loss: 0.02, acc.: 100.00%] [Q loss: 1.15] [G loss: 0.04]\n",
      "810 [D loss: 0.02, acc.: 100.00%] [Q loss: 1.12] [G loss: 0.16]\n",
      "811 [D loss: 0.04, acc.: 100.00%] [Q loss: 0.85] [G loss: 0.05]\n",
      "812 [D loss: 0.03, acc.: 100.00%] [Q loss: 0.92] [G loss: 0.02]\n",
      "813 [D loss: 0.02, acc.: 100.00%] [Q loss: 1.11] [G loss: 0.02]\n",
      "814 [D loss: 0.01, acc.: 100.00%] [Q loss: 1.06] [G loss: 0.03]\n",
      "815 [D loss: 0.08, acc.: 99.22%] [Q loss: 1.37] [G loss: 0.06]\n",
      "816 [D loss: 0.02, acc.: 100.00%] [Q loss: 1.50] [G loss: 0.08]\n",
      "817 [D loss: 0.04, acc.: 100.00%] [Q loss: 0.81] [G loss: 0.08]\n",
      "818 [D loss: 0.02, acc.: 100.00%] [Q loss: 0.87] [G loss: 0.03]\n",
      "819 [D loss: 0.01, acc.: 100.00%] [Q loss: 1.10] [G loss: 0.05]\n",
      "820 [D loss: 0.02, acc.: 100.00%] [Q loss: 0.76] [G loss: 0.05]\n",
      "821 [D loss: 0.01, acc.: 100.00%] [Q loss: 0.84] [G loss: 0.06]\n",
      "822 [D loss: 0.02, acc.: 100.00%] [Q loss: 0.87] [G loss: 0.03]\n",
      "823 [D loss: 0.02, acc.: 100.00%] [Q loss: 1.05] [G loss: 0.04]\n",
      "824 [D loss: 0.03, acc.: 100.00%] [Q loss: 1.21] [G loss: 0.03]\n",
      "825 [D loss: 0.02, acc.: 99.61%] [Q loss: 1.21] [G loss: 0.07]\n",
      "826 [D loss: 0.03, acc.: 100.00%] [Q loss: 0.97] [G loss: 0.05]\n",
      "827 [D loss: 0.01, acc.: 100.00%] [Q loss: 1.04] [G loss: 0.02]\n",
      "828 [D loss: 0.03, acc.: 100.00%] [Q loss: 1.02] [G loss: 0.03]\n",
      "829 [D loss: 0.01, acc.: 100.00%] [Q loss: 0.98] [G loss: 0.08]\n",
      "830 [D loss: 0.02, acc.: 100.00%] [Q loss: 0.97] [G loss: 0.09]\n",
      "831 [D loss: 0.02, acc.: 100.00%] [Q loss: 1.18] [G loss: 0.14]\n",
      "832 [D loss: 0.03, acc.: 100.00%] [Q loss: 1.25] [G loss: 0.01]\n",
      "833 [D loss: 0.03, acc.: 100.00%] [Q loss: 1.23] [G loss: 0.02]\n",
      "834 [D loss: 0.04, acc.: 100.00%] [Q loss: 1.51] [G loss: 0.04]\n",
      "835 [D loss: 0.10, acc.: 100.00%] [Q loss: 1.16] [G loss: 0.07]\n",
      "836 [D loss: 0.01, acc.: 100.00%] [Q loss: 1.61] [G loss: 0.04]\n",
      "837 [D loss: 0.02, acc.: 100.00%] [Q loss: 1.48] [G loss: 0.10]\n",
      "838 [D loss: 0.04, acc.: 99.61%] [Q loss: 1.33] [G loss: 0.08]\n",
      "839 [D loss: 0.03, acc.: 100.00%] [Q loss: 1.43] [G loss: 0.07]\n",
      "840 [D loss: 0.05, acc.: 99.61%] [Q loss: 1.56] [G loss: 0.06]\n",
      "841 [D loss: 0.01, acc.: 100.00%] [Q loss: 1.62] [G loss: 0.09]\n",
      "842 [D loss: 0.01, acc.: 100.00%] [Q loss: 1.37] [G loss: 0.15]\n",
      "843 [D loss: 0.06, acc.: 99.61%] [Q loss: 1.59] [G loss: 0.06]\n",
      "844 [D loss: 0.02, acc.: 100.00%] [Q loss: 1.35] [G loss: 0.08]\n",
      "845 [D loss: 0.03, acc.: 100.00%] [Q loss: 1.13] [G loss: 0.05]\n",
      "846 [D loss: 0.02, acc.: 100.00%] [Q loss: 1.41] [G loss: 0.07]\n",
      "847 [D loss: 0.06, acc.: 99.22%] [Q loss: 0.62] [G loss: 0.03]\n",
      "848 [D loss: 0.02, acc.: 100.00%] [Q loss: 0.97] [G loss: 0.04]\n",
      "849 [D loss: 0.02, acc.: 100.00%] [Q loss: 1.07] [G loss: 0.03]\n",
      "850 [D loss: 0.02, acc.: 100.00%] [Q loss: 1.28] [G loss: 0.09]\n",
      "851 [D loss: 0.01, acc.: 100.00%] [Q loss: 1.52] [G loss: 0.03]\n",
      "852 [D loss: 0.01, acc.: 100.00%] [Q loss: 1.26] [G loss: 0.08]\n",
      "853 [D loss: 0.02, acc.: 100.00%] [Q loss: 1.17] [G loss: 0.01]\n",
      "854 [D loss: 0.01, acc.: 100.00%] [Q loss: 1.55] [G loss: 0.02]\n",
      "855 [D loss: 0.03, acc.: 100.00%] [Q loss: 1.61] [G loss: 0.04]\n",
      "856 [D loss: 0.05, acc.: 100.00%] [Q loss: 1.77] [G loss: 0.06]\n",
      "857 [D loss: 0.05, acc.: 99.61%] [Q loss: 1.00] [G loss: 0.17]\n",
      "858 [D loss: 0.02, acc.: 100.00%] [Q loss: 0.97] [G loss: 0.08]\n",
      "859 [D loss: 0.02, acc.: 100.00%] [Q loss: 1.34] [G loss: 0.06]\n",
      "860 [D loss: 0.02, acc.: 100.00%] [Q loss: 1.21] [G loss: 0.08]\n",
      "861 [D loss: 0.01, acc.: 100.00%] [Q loss: 1.12] [G loss: 0.18]\n",
      "862 [D loss: 0.03, acc.: 100.00%] [Q loss: 1.47] [G loss: 0.06]\n",
      "863 [D loss: 0.02, acc.: 100.00%] [Q loss: 1.53] [G loss: 0.03]\n",
      "864 [D loss: 0.03, acc.: 100.00%] [Q loss: 1.00] [G loss: 0.09]\n",
      "865 [D loss: 0.03, acc.: 99.61%] [Q loss: 1.69] [G loss: 0.08]\n",
      "866 [D loss: 0.08, acc.: 99.61%] [Q loss: 1.73] [G loss: 0.10]\n",
      "867 [D loss: 0.01, acc.: 100.00%] [Q loss: 2.00] [G loss: 0.04]\n",
      "868 [D loss: 0.16, acc.: 96.09%] [Q loss: 4.92] [G loss: 0.05]\n",
      "869 [D loss: 7.57, acc.: 1.56%] [Q loss: 0.00] [G loss: 0.41]\n",
      "870 [D loss: 4.22, acc.: 50.00%] [Q loss: 0.00] [G loss: 0.22]\n",
      "871 [D loss: 1.37, acc.: 53.12%] [Q loss: 0.81] [G loss: 0.08]\n",
      "872 [D loss: 0.41, acc.: 80.08%] [Q loss: 0.50] [G loss: 0.06]\n",
      "873 [D loss: 0.29, acc.: 84.77%] [Q loss: 0.20] [G loss: 0.14]\n",
      "874 [D loss: 0.12, acc.: 98.05%] [Q loss: 0.16] [G loss: 0.02]\n",
      "875 [D loss: 0.06, acc.: 100.00%] [Q loss: 0.22] [G loss: 0.01]\n",
      "876 [D loss: 0.06, acc.: 100.00%] [Q loss: 0.29] [G loss: 0.01]\n",
      "877 [D loss: 0.08, acc.: 100.00%] [Q loss: 0.36] [G loss: 0.01]\n",
      "878 [D loss: 0.07, acc.: 100.00%] [Q loss: 0.46] [G loss: 0.00]\n",
      "879 [D loss: 0.07, acc.: 99.61%] [Q loss: 0.48] [G loss: 0.01]\n",
      "880 [D loss: 0.03, acc.: 100.00%] [Q loss: 0.54] [G loss: 0.01]\n",
      "881 [D loss: 0.05, acc.: 100.00%] [Q loss: 0.53] [G loss: 0.01]\n",
      "882 [D loss: 0.04, acc.: 99.61%] [Q loss: 0.46] [G loss: 0.04]\n",
      "883 [D loss: 0.06, acc.: 99.61%] [Q loss: 0.57] [G loss: 0.01]\n",
      "884 [D loss: 0.03, acc.: 100.00%] [Q loss: 0.66] [G loss: 0.01]\n",
      "885 [D loss: 0.03, acc.: 100.00%] [Q loss: 0.62] [G loss: 0.01]\n",
      "886 [D loss: 0.02, acc.: 100.00%] [Q loss: 0.67] [G loss: 0.03]\n",
      "887 [D loss: 0.03, acc.: 100.00%] [Q loss: 0.69] [G loss: 0.01]\n",
      "888 [D loss: 0.03, acc.: 100.00%] [Q loss: 0.74] [G loss: 0.02]\n",
      "889 [D loss: 0.02, acc.: 100.00%] [Q loss: 0.70] [G loss: 0.03]\n",
      "890 [D loss: 0.02, acc.: 99.61%] [Q loss: 0.68] [G loss: 0.01]\n",
      "891 [D loss: 0.02, acc.: 100.00%] [Q loss: 0.65] [G loss: 0.02]\n",
      "892 [D loss: 0.02, acc.: 100.00%] [Q loss: 0.61] [G loss: 0.05]\n",
      "893 [D loss: 0.02, acc.: 100.00%] [Q loss: 0.67] [G loss: 0.02]\n",
      "894 [D loss: 0.03, acc.: 100.00%] [Q loss: 0.59] [G loss: 0.05]\n",
      "895 [D loss: 0.02, acc.: 100.00%] [Q loss: 0.65] [G loss: 0.03]\n",
      "896 [D loss: 0.03, acc.: 100.00%] [Q loss: 0.56] [G loss: 0.01]\n",
      "897 [D loss: 0.02, acc.: 100.00%] [Q loss: 0.52] [G loss: 0.01]\n",
      "898 [D loss: 0.02, acc.: 100.00%] [Q loss: 0.52] [G loss: 0.01]\n",
      "899 [D loss: 0.02, acc.: 100.00%] [Q loss: 0.57] [G loss: 0.01]\n",
      "900 [D loss: 0.01, acc.: 100.00%] [Q loss: 0.59] [G loss: 0.01]\n",
      "901 [D loss: 0.03, acc.: 100.00%] [Q loss: 0.48] [G loss: 0.04]\n",
      "902 [D loss: 0.02, acc.: 100.00%] [Q loss: 0.45] [G loss: 0.04]\n",
      "903 [D loss: 0.03, acc.: 100.00%] [Q loss: 0.49] [G loss: 0.01]\n",
      "904 [D loss: 0.03, acc.: 100.00%] [Q loss: 0.48] [G loss: 0.01]\n",
      "905 [D loss: 0.02, acc.: 100.00%] [Q loss: 0.52] [G loss: 0.01]\n",
      "906 [D loss: 0.01, acc.: 100.00%] [Q loss: 0.50] [G loss: 0.02]\n",
      "907 [D loss: 0.02, acc.: 100.00%] [Q loss: 0.51] [G loss: 0.02]\n",
      "908 [D loss: 0.02, acc.: 100.00%] [Q loss: 0.45] [G loss: 0.03]\n",
      "909 [D loss: 0.03, acc.: 100.00%] [Q loss: 0.46] [G loss: 0.02]\n",
      "910 [D loss: 0.02, acc.: 100.00%] [Q loss: 0.44] [G loss: 0.02]\n",
      "911 [D loss: 0.02, acc.: 100.00%] [Q loss: 0.43] [G loss: 0.01]\n",
      "912 [D loss: 0.02, acc.: 100.00%] [Q loss: 0.36] [G loss: 0.04]\n",
      "913 [D loss: 0.03, acc.: 100.00%] [Q loss: 0.34] [G loss: 0.02]\n",
      "914 [D loss: 0.02, acc.: 100.00%] [Q loss: 0.40] [G loss: 0.02]\n",
      "915 [D loss: 0.02, acc.: 100.00%] [Q loss: 0.36] [G loss: 0.02]\n",
      "916 [D loss: 0.02, acc.: 100.00%] [Q loss: 0.37] [G loss: 0.01]\n",
      "917 [D loss: 0.02, acc.: 100.00%] [Q loss: 0.38] [G loss: 0.03]\n",
      "918 [D loss: 0.02, acc.: 100.00%] [Q loss: 0.37] [G loss: 0.02]\n",
      "919 [D loss: 0.02, acc.: 100.00%] [Q loss: 0.36] [G loss: 0.02]\n",
      "920 [D loss: 0.02, acc.: 100.00%] [Q loss: 0.36] [G loss: 0.04]\n",
      "921 [D loss: 0.02, acc.: 100.00%] [Q loss: 0.35] [G loss: 0.01]\n",
      "922 [D loss: 0.04, acc.: 100.00%] [Q loss: 0.36] [G loss: 0.01]\n",
      "923 [D loss: 0.02, acc.: 100.00%] [Q loss: 0.40] [G loss: 0.02]\n",
      "924 [D loss: 0.02, acc.: 100.00%] [Q loss: 0.40] [G loss: 0.03]\n",
      "925 [D loss: 0.02, acc.: 100.00%] [Q loss: 0.38] [G loss: 0.03]\n",
      "926 [D loss: 0.02, acc.: 100.00%] [Q loss: 0.38] [G loss: 0.04]\n",
      "927 [D loss: 0.02, acc.: 100.00%] [Q loss: 0.35] [G loss: 0.03]\n",
      "928 [D loss: 0.03, acc.: 100.00%] [Q loss: 0.39] [G loss: 0.03]\n",
      "929 [D loss: 0.02, acc.: 100.00%] [Q loss: 0.44] [G loss: 0.02]\n",
      "930 [D loss: 0.03, acc.: 100.00%] [Q loss: 0.42] [G loss: 0.03]\n",
      "931 [D loss: 0.03, acc.: 100.00%] [Q loss: 0.54] [G loss: 0.03]\n",
      "932 [D loss: 0.02, acc.: 100.00%] [Q loss: 0.48] [G loss: 0.05]\n",
      "933 [D loss: 0.02, acc.: 100.00%] [Q loss: 0.46] [G loss: 0.02]\n",
      "934 [D loss: 0.02, acc.: 100.00%] [Q loss: 0.48] [G loss: 0.05]\n",
      "935 [D loss: 0.02, acc.: 100.00%] [Q loss: 0.45] [G loss: 0.06]\n",
      "936 [D loss: 0.02, acc.: 100.00%] [Q loss: 0.52] [G loss: 0.04]\n",
      "937 [D loss: 0.03, acc.: 100.00%] [Q loss: 0.56] [G loss: 0.04]\n",
      "938 [D loss: 0.02, acc.: 100.00%] [Q loss: 0.54] [G loss: 0.07]\n",
      "939 [D loss: 0.03, acc.: 100.00%] [Q loss: 0.61] [G loss: 0.02]\n",
      "940 [D loss: 0.03, acc.: 100.00%] [Q loss: 0.59] [G loss: 0.08]\n",
      "941 [D loss: 0.04, acc.: 100.00%] [Q loss: 0.70] [G loss: 0.07]\n",
      "942 [D loss: 0.04, acc.: 100.00%] [Q loss: 0.62] [G loss: 0.03]\n",
      "943 [D loss: 0.03, acc.: 100.00%] [Q loss: 0.66] [G loss: 0.07]\n",
      "944 [D loss: 0.03, acc.: 100.00%] [Q loss: 0.62] [G loss: 0.05]\n",
      "945 [D loss: 0.04, acc.: 100.00%] [Q loss: 0.89] [G loss: 0.06]\n",
      "946 [D loss: 0.03, acc.: 99.61%] [Q loss: 1.02] [G loss: 0.04]\n",
      "947 [D loss: 0.04, acc.: 99.61%] [Q loss: 0.81] [G loss: 0.09]\n",
      "948 [D loss: 0.05, acc.: 100.00%] [Q loss: 0.85] [G loss: 0.15]\n",
      "949 [D loss: 0.03, acc.: 100.00%] [Q loss: 1.00] [G loss: 0.02]\n",
      "950 [D loss: 0.05, acc.: 100.00%] [Q loss: 1.13] [G loss: 0.08]\n",
      "951 [D loss: 0.06, acc.: 100.00%] [Q loss: 0.92] [G loss: 0.09]\n",
      "952 [D loss: 0.04, acc.: 100.00%] [Q loss: 0.93] [G loss: 0.04]\n",
      "953 [D loss: 0.12, acc.: 99.22%] [Q loss: 1.41] [G loss: 0.09]\n",
      "954 [D loss: 0.06, acc.: 100.00%] [Q loss: 1.33] [G loss: 0.08]\n",
      "955 [D loss: 0.06, acc.: 100.00%] [Q loss: 1.11] [G loss: 0.08]\n",
      "956 [D loss: 0.09, acc.: 98.83%] [Q loss: 1.90] [G loss: 0.05]\n",
      "957 [D loss: 0.05, acc.: 100.00%] [Q loss: 1.79] [G loss: 0.13]\n",
      "958 [D loss: 0.06, acc.: 100.00%] [Q loss: 1.01] [G loss: 0.12]\n",
      "959 [D loss: 0.08, acc.: 99.61%] [Q loss: 1.73] [G loss: 0.13]\n",
      "960 [D loss: 0.07, acc.: 100.00%] [Q loss: 1.54] [G loss: 0.08]\n",
      "961 [D loss: 0.05, acc.: 100.00%] [Q loss: 1.22] [G loss: 0.08]\n",
      "962 [D loss: 0.08, acc.: 99.22%] [Q loss: 2.18] [G loss: 0.07]\n",
      "963 [D loss: 0.63, acc.: 65.62%] [Q loss: 5.87] [G loss: 0.19]\n",
      "964 [D loss: 4.11, acc.: 0.00%] [Q loss: 0.53] [G loss: 0.32]\n",
      "965 [D loss: 0.04, acc.: 100.00%] [Q loss: 2.43] [G loss: 0.02]\n",
      "966 [D loss: 0.22, acc.: 94.53%] [Q loss: 1.47] [G loss: 0.05]\n",
      "967 [D loss: 0.27, acc.: 89.45%] [Q loss: 2.31] [G loss: 0.06]\n",
      "968 [D loss: 0.19, acc.: 96.48%] [Q loss: 1.57] [G loss: 0.03]\n",
      "969 [D loss: 0.13, acc.: 98.83%] [Q loss: 1.37] [G loss: 0.11]\n",
      "970 [D loss: 0.08, acc.: 99.61%] [Q loss: 1.24] [G loss: 0.07]\n",
      "971 [D loss: 0.04, acc.: 100.00%] [Q loss: 1.18] [G loss: 0.04]\n",
      "972 [D loss: 0.06, acc.: 99.61%] [Q loss: 1.02] [G loss: 0.02]\n",
      "973 [D loss: 0.03, acc.: 100.00%] [Q loss: 1.17] [G loss: 0.03]\n",
      "974 [D loss: 0.03, acc.: 100.00%] [Q loss: 1.07] [G loss: 0.04]\n",
      "975 [D loss: 0.04, acc.: 100.00%] [Q loss: 1.02] [G loss: 0.03]\n",
      "976 [D loss: 0.04, acc.: 100.00%] [Q loss: 1.01] [G loss: 0.03]\n",
      "977 [D loss: 0.03, acc.: 100.00%] [Q loss: 0.97] [G loss: 0.13]\n",
      "978 [D loss: 0.04, acc.: 100.00%] [Q loss: 1.04] [G loss: 0.12]\n",
      "979 [D loss: 0.03, acc.: 100.00%] [Q loss: 1.11] [G loss: 0.05]\n",
      "980 [D loss: 0.04, acc.: 100.00%] [Q loss: 0.91] [G loss: 0.17]\n",
      "981 [D loss: 0.02, acc.: 100.00%] [Q loss: 0.95] [G loss: 0.06]\n",
      "982 [D loss: 0.03, acc.: 100.00%] [Q loss: 1.00] [G loss: 0.02]\n",
      "983 [D loss: 0.01, acc.: 100.00%] [Q loss: 1.18] [G loss: 0.04]\n",
      "984 [D loss: 0.02, acc.: 100.00%] [Q loss: 1.09] [G loss: 0.06]\n",
      "985 [D loss: 0.04, acc.: 100.00%] [Q loss: 1.14] [G loss: 0.05]\n",
      "986 [D loss: 0.04, acc.: 100.00%] [Q loss: 1.05] [G loss: 0.09]\n",
      "987 [D loss: 0.02, acc.: 100.00%] [Q loss: 1.15] [G loss: 0.07]\n",
      "988 [D loss: 0.02, acc.: 100.00%] [Q loss: 1.28] [G loss: 0.06]\n",
      "989 [D loss: 0.03, acc.: 100.00%] [Q loss: 1.07] [G loss: 0.04]\n",
      "990 [D loss: 0.02, acc.: 100.00%] [Q loss: 1.16] [G loss: 0.09]\n",
      "991 [D loss: 0.05, acc.: 100.00%] [Q loss: 1.41] [G loss: 0.12]\n",
      "992 [D loss: 0.05, acc.: 99.22%] [Q loss: 1.30] [G loss: 0.07]\n",
      "993 [D loss: 0.01, acc.: 100.00%] [Q loss: 1.20] [G loss: 0.05]\n",
      "994 [D loss: 0.04, acc.: 100.00%] [Q loss: 1.17] [G loss: 0.07]\n",
      "995 [D loss: 0.03, acc.: 100.00%] [Q loss: 1.31] [G loss: 0.34]\n",
      "996 [D loss: 0.03, acc.: 100.00%] [Q loss: 1.39] [G loss: 0.06]\n",
      "997 [D loss: 0.06, acc.: 100.00%] [Q loss: 1.13] [G loss: 0.10]\n",
      "998 [D loss: 0.05, acc.: 100.00%] [Q loss: 1.33] [G loss: 0.03]\n",
      "999 [D loss: 0.03, acc.: 100.00%] [Q loss: 1.31] [G loss: 0.07]\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, multiply, concatenate\n",
    "from keras.layers import BatchNormalization, Activation, Embedding, ZeroPadding2D, Lambda\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import keras.backend as K\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class INFOGAN():\n",
    "    def __init__(self):\n",
    "        self.img_rows = 28\n",
    "        self.img_cols = 28\n",
    "        self.channels = 1\n",
    "        self.num_classes = 10\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "        self.latent_dim = 72\n",
    "\n",
    "\n",
    "        optimizer = Adam(0.0002, 0.5)\n",
    "        losses = ['binary_crossentropy', self.mutual_info_loss]\n",
    "\n",
    "        # Build and the discriminator and recognition network\n",
    "        self.discriminator, self.auxilliary = self.build_disk_and_q_net()\n",
    "\n",
    "        self.discriminator.compile(loss=['binary_crossentropy'],\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "        # Build and compile the recognition network Q\n",
    "        self.auxilliary.compile(loss=[self.mutual_info_loss],\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "        # Build the generator\n",
    "        self.generator = self.build_generator()\n",
    "\n",
    "        # The generator takes noise and the target label as input\n",
    "        # and generates the corresponding digit of that label\n",
    "        gen_input = Input(shape=(self.latent_dim,))\n",
    "        img = self.generator(gen_input)\n",
    "\n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "\n",
    "        # The discriminator takes generated image as input and determines validity\n",
    "        valid = self.discriminator(img)\n",
    "        # The recognition network produces the label\n",
    "        target_label = self.auxilliary(img)\n",
    "\n",
    "        # The combined model  (stacked generator and discriminator)\n",
    "        self.combined = Model(gen_input, [valid, target_label])\n",
    "        self.combined.compile(loss=losses,\n",
    "            optimizer=optimizer)\n",
    "\n",
    "\n",
    "    def build_generator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Dense(128 * 7 * 7, activation=\"relu\", input_dim=self.latent_dim))\n",
    "        model.add(Reshape((7, 7, 128)))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(UpSampling2D())\n",
    "        model.add(Conv2D(128, kernel_size=3, padding=\"same\"))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(UpSampling2D())\n",
    "        model.add(Conv2D(64, kernel_size=3, padding=\"same\"))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Conv2D(self.channels, kernel_size=3, padding='same'))\n",
    "        model.add(Activation(\"tanh\"))\n",
    "\n",
    "        gen_input = Input(shape=(self.latent_dim,))\n",
    "        img = model(gen_input)\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        return Model(gen_input, img)\n",
    "\n",
    "\n",
    "    def build_disk_and_q_net(self):\n",
    "\n",
    "        img = Input(shape=self.img_shape)\n",
    "\n",
    "        # Shared layers between discriminator and recognition network\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(64, kernel_size=3, strides=2, input_shape=self.img_shape, padding=\"same\"))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(128, kernel_size=3, strides=2, padding=\"same\"))\n",
    "        model.add(ZeroPadding2D(padding=((0,1),(0,1))))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Conv2D(256, kernel_size=3, strides=2, padding=\"same\"))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Conv2D(512, kernel_size=3, strides=2, padding=\"same\"))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Flatten())\n",
    "\n",
    "        img_embedding = model(img)\n",
    "\n",
    "        # Discriminator\n",
    "        validity = Dense(1, activation='sigmoid')(img_embedding)\n",
    "\n",
    "        # Recognition\n",
    "        q_net = Dense(128, activation='relu')(img_embedding)\n",
    "        label = Dense(self.num_classes, activation='softmax')(q_net)\n",
    "\n",
    "        # Return discriminator and recognition network\n",
    "        return Model(img, validity), Model(img, label)\n",
    "\n",
    "\n",
    "    def mutual_info_loss(self, c, c_given_x):\n",
    "        \"\"\"The mutual information metric we aim to minimize\"\"\"\n",
    "        eps = 1e-8\n",
    "        conditional_entropy = K.mean(- K.sum(K.log(c_given_x + eps) * c, axis=1))\n",
    "        entropy = K.mean(- K.sum(K.log(c + eps) * c, axis=1))\n",
    "\n",
    "        return conditional_entropy + entropy\n",
    "\n",
    "    def sample_generator_input(self, batch_size):\n",
    "        # Generator inputs\n",
    "        sampled_noise = np.random.normal(0, 1, (batch_size, 62))\n",
    "        sampled_labels = np.random.randint(0, self.num_classes, batch_size).reshape(-1, 1)\n",
    "        sampled_labels = to_categorical(sampled_labels, num_classes=self.num_classes)\n",
    "\n",
    "        return sampled_noise, sampled_labels\n",
    "\n",
    "    def train(self, epochs, batch_size=128, sample_interval=50):\n",
    "\n",
    "        # Load the dataset\n",
    "        (X_train, y_train), (_, _) = mnist.load_data()\n",
    "\n",
    "        # Rescale -1 to 1\n",
    "        X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n",
    "        X_train = np.expand_dims(X_train, axis=3)\n",
    "        y_train = y_train.reshape(-1, 1)\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = np.ones((batch_size, 1))\n",
    "        fake = np.zeros((batch_size, 1))\n",
    "\n",
    "        g_losses = []\n",
    "        q_losses = []\n",
    "        d_losses = []\n",
    "\n",
    "        d_accuracy = []\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            # Select a random half batch of images\n",
    "            idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "            imgs = X_train[idx]\n",
    "\n",
    "            # Sample noise and categorical labels\n",
    "            sampled_noise, sampled_labels = self.sample_generator_input(batch_size)\n",
    "            gen_input = np.concatenate((sampled_noise, sampled_labels), axis=1)\n",
    "\n",
    "            # Generate a half batch of new images\n",
    "            gen_imgs = self.generator.predict(gen_input)\n",
    "\n",
    "            # Train on real and generated data\n",
    "            d_loss_real = self.discriminator.train_on_batch(imgs, valid)\n",
    "            d_loss_fake = self.discriminator.train_on_batch(gen_imgs, fake)\n",
    "\n",
    "            # Avg. loss\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Generator and Q-network\n",
    "            # ---------------------\n",
    "\n",
    "            g_loss = self.combined.train_on_batch(gen_input, [valid, sampled_labels])\n",
    "\n",
    "            # Plot the progress\n",
    "            print (\"%d [D loss: %.2f, acc.: %.2f%%] [Q loss: %.2f] [G loss: %.2f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss[1], g_loss[2]))\n",
    "\n",
    "            g_losses.append(g_loss[2])\n",
    "            q_losses.append(g_loss[1])\n",
    "            d_losses.append(d_loss[0])\n",
    "            d_accuracy.append(100*d_loss[1])\n",
    "\n",
    "            # If at save interval => save generated image samples\n",
    "            if epoch % sample_interval == 0:\n",
    "                self.sample_images(epoch)\n",
    "        return g_losses,q_losses,d_losses,d_accuracy\n",
    "    def sample_images(self, epoch):\n",
    "        r, c = 10, 10\n",
    "\n",
    "        fig, axs = plt.subplots(r, c)\n",
    "        for i in range(c):\n",
    "            sampled_noise, _ = self.sample_generator_input(c)\n",
    "            label = to_categorical(np.full(fill_value=i, shape=(r,1)), num_classes=self.num_classes)\n",
    "            gen_input = np.concatenate((sampled_noise, label), axis=1)\n",
    "            gen_imgs = self.generator.predict(gen_input)\n",
    "            gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "            for j in range(r):\n",
    "                axs[j,i].imshow(gen_imgs[j,:,:,0], cmap='gray')\n",
    "                axs[j,i].axis('off')\n",
    "        # fig.savefig(\"images/%d.png\" % epoch)\n",
    "        plt.close()\n",
    "\n",
    "    def save_model(self):\n",
    "\n",
    "        def save(model, model_name):\n",
    "            model_path = \"saved_model/%s.json\" % model_name\n",
    "            weights_path = \"saved_model/%s_weights.hdf5\" % model_name\n",
    "            options = {\"file_arch\": model_path,\n",
    "                        \"file_weight\": weights_path}\n",
    "            json_string = model.to_json()\n",
    "            open(options['file_arch'], 'w').write(json_string)\n",
    "            model.save_weights(options['file_weight'])\n",
    "\n",
    "        save(self.generator, \"generator\")\n",
    "        save(self.discriminator, \"discriminator\")\n",
    "\n",
    "\n",
    "infogan = INFOGAN()\n",
    "g_losses,q_losses,d_losses,d_accuracy = infogan.train(epochs=1000, batch_size=128, sample_interval=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "dSzoIX9N8hcU",
    "outputId": "3f8fc14d-cc6e-4581-cfa4-5f15dd28d99e"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(d_losses)\n",
    "plt.plot(g_losses)\n",
    "# plt.plot(q_losses)\n",
    "\n",
    "plt.title('Loss per epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['Disc Loss', 'Gen Loss','Q-net Loss'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QNOYm7XxMN0a"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "InfoGAN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
